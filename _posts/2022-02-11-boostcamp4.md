---
layout: post
title: "[부스트캠프 AI Tech 3기] 4주차 정리 & 회고"
categories:
  - BoostCamp
tags:
  - 부스트캠프
  - AI
  - Deep Learning
  - CNN
  - RNN
  - GAN
  - MLP
  - Transformer
  - Data Visualization
---

---
![Untitled](/assets/img/AITech로고.png)

# ✏️학습정리

## Historic Review of Deep Learning

- 딥러닝은 방대한 분야 → 장님이 코끼리 만지는 격
- 무엇이 좋은 딥러너의 요소인가
    - 구현 스킬, 수학 스킬(선형대수, 확률론), 논문 따라가기
- 인공지능 → 사람의 지능을 모방하는 것
- 인공지능 > 기계학습 > 딥러닝
- 딥러닝 연구 ≠ 인공지능 전체 연구
- 딥러닝에 중요한 요소들: 염두해 두면 논문 읽을때도 핵심을 파악하기 쉬움
    - 학습할 데이터: 문제의 장르에 따라 다른 형식
    - 학습시킬 모델
    - 모델을 평가할 손실함수: 이걸 줄인다고 우리가 원한 결과에 도달하는 보장은 없으니 주의
    - 손실함수를 최소화할 알고리즘
- 딥러닝의 역사
    - 2012 - AlexNet: 이미지 대회에서 딥러닝이 첫음으로 1위 한 모델, 패러다임 시프트
    - 2013 - DQN: 강화학습의 딥러닝 적용판, 후에 알파고로 발전
    - 2014 - Encoder/Decoder, Adam: 이 이후로 NLP 판도가 바뀜, Adam은 Optimizer 중 가장 결과가 잘 나오는 편
    - 2015 - GAN, ResNet: 딥러닝에서 중요한 이론(Generator, Discriminator), ResNet은 깊은 네트워크에서 우회법을 제안한 패러다임 시프트
    - 2017 - Transformer: Attention is all you need, RNN의 필수구조가 된 패러다임 시프트
    - 2018 - Bert: 파인 튜닝을 활용하는 패러다임 시프트
    - 2019 - GPT-X: 초 거대 패러미터를 활용한 학습모델
    - 2020 - Self-Supervised Learning: 분류 문제에서 주어진 데이터 말고도 불명확한 데이터까지 같이 학습에 활용하는 방법론

## Multi-Layer Perceptron

- 인공 신경망 - 뇌 신경망을 모방했다는 설명 → 굳이 여기에 집착할 필요는 없음
    - 삼라만상을 그리는 함수를 근사 & 모방하기 위한 모델 → 좀 더 정확한 설명
- 손실함수를 최소화하기 위해 역전파를 이용
- 세상이 단순히 선형함수로 이루어져 있지 않다 → 비선형 변형 함수를 레이어에 포함
- 히든 레이어 1개의 신경망은 이론적으로 삼라만상의 함수를 근사할 수 있다 → 어디까지나 가능성
- 레이어 하나론 힘들고, 여러개를 겹치는 것이 MLP

## Optimization

- 최적화 용어들이 많음 → 확실히 알아둬야 후일 미스커뮤니케이션 방지
- generalization
    - 일반화 성능을 높이는게 주 목적
        - 트레이닝 에러가 줄어든다는 것이 테스트 에러도 줄이지 않는다
        - 갭(generalization gap)이 클 수록 overfitting
- cross-validation
    - 학습 데이터를 k개로 나눠, 일부만 학습시키고, 나머지로 validation
    - 테스트 데이터는 절대 학습에 쓰면 안됨
- bias and variance
    - low bias, low variance를 목표
    - cost = bias + variance + noise, 따라서 어느 하나를 최소화 하면 다른 쪽이 커진다. 균형이 중요
- Bootstrapping
    - 한 데이터셋을 랜덤 샘플링 하여 다양한 서브 데이터셋으로 학습을 도모하는 방법론
- Bagging
    - Bootstrapping aggregating - ensemble - parallel
- Boosting
    - week learner 여러개로 하나의 strong learner 구축 - sequential
- Gradient Descent Methods
    - Stochastic - 한 샘플로 진행
    - Mini-batch - 한 데이터셋의 서브 데이터셋으로 진행
    - Batch - 전체 데이터셋으로 진행
    - 대부분 미니 배치 활용
    - 배치 사이즈가 꽤 중요 → 배치 사이즈가 줄어들면 generalize performance 보통 상승
- Other GD Methods
    - Momentum - 한 번 흘러가는 방향을 유지하게 하여 트레이닝을 스무스하게
    - Nestrov Accelerated Gradient - Momentum의 문제점을 수정
    - Adagrad - 지금까지의 패러미터 변화를 측정하여 그것을 기반으로 learning rate 수정
    - Adadelta - Adagrad와 달리 일정 구간의 타임라인 구간을 정해서? → 많이 쓰이진 않음
    - RMSprop - Adagrad에 스텝사이즈 추가
    - Adam - Momentum과 Adaptive learning rate approach 를 잘 결합 → 주류
- Regularization
    - 학습을 좀 방해하여 generalization performance 높이는 전략들
    - Early Stopping - validation data와 비교하여 퍼포먼스가 둘 다 높은 부분에서 학습 중지
    - Parameter Norm Penalty - 좀 더 부드러운 함수가 더 나을거란 관점
    - Data Augmentation - 데이터가 많을수록 좋다 → 기존 데이터를 활용해 변형하여 많은 배리에이션을 창출
    - Noise Robustness - 입력 데이터와 네트워크 가중치에 노이즈를 넣어 퍼포먼스 상승
    - Label Smoothing - decision boundary를 smoothing 하여 성능 업(Mixup, Cutmix 등)
    - Dropout - 각 레이어마다 포워딩 시 랜덤하게 뉴런들을 탈락시켜 성능 업
    - Batch Normalization - 논란 많음, 레이어 별로 값을 통계학적으로 줄여 성능 업

## Convolution Neural Network

- 전체 이미지에 국소적 크기의 필터를 순차적으로 대입하여 연산하는 방법
- 스텝 바이 스텝으로 필터라는 도장을 찍어나가는 느낌으로 연상하면 좋다
- 다양한 필터들이 다양한 결과물을 만들어낸다
- 일반적으로 RGB 이미지라 2d 이미지가 3겹인 텐서를 다룬다 생각하면 된다
- 필터도 같이 3겹으로 만든다 → 결과물은 2d가 됨
- 여러겹의 결과물을 원한다 → 필터를 여러개 쓴다
- CNN의 고전적 구조
    - convolution & pooling layer : feature extraction
    - Fully connected layer : descion(classification) → 최근엔 이 부분의 크기를 줄이고 있음
- Stride - 커널(필터)의 도장찍는 보폭, 고전적으로 1로 설정
- Padding - 컨볼루션의 결과로 전체 크기가 줄어드는걸 막기 위해 사전에 가장자리를 특정 수로 채우기 → 제로패딩
- 1x1 convolution - 1x1xh 사이즈의 커널을 h보다 적은 양으로 적용해 차원을 줄이는 것 → 더 깊게 쌓아도 패러미터가 적어서 퍼포먼스 업 (bottleneck architecture)

## History of CNN

- ILSVRC 에서 1등한 역대 네트워크들 리뷰
- AlexNet : GPU 파워의 부족으로 모델을 두 개로 쪼개 두 개의 GPU에 학습, 활성함수로 ReLU 사용, Overlapping pooling, data augmentation, dropout 등 사용, 사실상의 패러다임 시프트
- Relu: Vanishing Gradient Problem 극복
- VGGNet : 3x3 conv 필터 활용,  3x3을 두 번 활용 하는 것이 5x5 하나 사용 한 것과 결과적으로 같으면서도 패러미터량이 줄어든다.
- GoogLeNet : 디멘션(채널)을 1x1conv으로 줄여서 패러미터량 최적화, inception blocks 활용하여 패러미터량 더더욱 줄이면서 퍼포먼스 업
- ResNet: generalize performance 이슈 → 네트워크가 깊을수록 오버피팅 이슈를 겪는다는 통념 → skip connection 구조로 혁신, batch norm 적용 순서에 따른 퍼포먼스 차이 이슈 있음
- DenseNet: ResNet과 달리 합연산이 아닌 concatenation을 하는 방식 → 중간에 차원 양을 조절

## Computer Vision Applications

- Semantic Segmentation
    - 어떤 이미지 한 장 내에서 각 픽셀의 라벨링을 하는 문제
    - 자율주행의 이미지 인식 등에 활용
    - 기존 CNN은 한 장의 이미지를 하나의 라벨과 결부시키는 구조 → 변형이 필요
    - Fully convolution Network: Dense layer를 없애고 그 자리에 convolution layer를 추가 → 패러미터량 변화는 없음
    - 이미지 크기 제한에 어느정도 자유로워짐
    - deconvolution(역연산, 엄밀히는 아님)
- Detection
    - R-CNN: 이미지 하나에서 다양한 요소들의 이미지를 뽑아내서 학습
    - SPPNet: 바운더리를 한 번만 세팅
    - Fast R-CNN: roi pooling으로 미리 세팅된 바운더리 내의 이미지 학습, 이후 바운더리 이동도 학습
    - Faster R-CNN: region proposal network 추가, 바운더리 설정을 학습, 미리 정해진 사이즈의 바운더리 박스로 세팅
    - YOLO: 이미지 하나에서 동시에 바운더리와 클래스 계산을 수행, 매우 빠름

## Recurrent Neural Network

- Sequential Model
    - 일상의 대부분 데이터는 시계열순
    - 정의상 입력의 차원을 알 수 없다
    - 인풋 크기에 구애받지 않고 구동되어야 한다
    - 시간이 뒤로 갈 수록 과거정보가 누적 → fixed time step으로 해결
    - Markov model - 바로 전 과거만 포함하여 계산 → 너무 많은 정보를 버리게 됨, joint distribution 표현엔 좋음
    - Latent autoregressive model - 하나의 hidden state가 과거 정보를 요약
- Recurrent Neural Network
    - 사실상 입력이 굉장히 많은 fully connected layer
    - 너무 과거의 정보는 미래에 반영되기 힘듬
    - Vanishing or Exploding gradient 문제 - 과거의 정보를 계속 누적해서 계산할 수록 그래디언트가 극단적으로 치닫게 됨
- LSTM
    - Long Short Term Memory
    - Cell state 가 계속 흘러가 전체 네트워크의 상태를 표현
    - Forget gate - 어떤 정보를 버릴 지 결정
    - Input Gate - 어떤 정보를 올릴 지 결정
    - Update Cell - 두 게이트에서 나온 정보를 기반으로 Cell state 업데이트
    - output Gate - 최신 Cell state로 어떤 값을 빼낼 지 결정
- GRU
    - Gated Recurrent Unit
    - Reset Gate
    - Update Gate
    - Cell state 없이 Hidden state를 아웃풋에 활용

## Transformer

- 기존의 RNN은 인풋 데이터의 길이가 불확실해서 모델링에 어려움을 겪었다
- Transtormer - attention 구조를 활용한 모델
- 내부는 여러 겹의 인코더와 여러 겹의 디코더로 이루어짐
- 재귀적이 아닌 한 번에 전체 시퀀스를 병렬 처리
- 인코더 - 셀프 어텐션 + 피드 포워드로 구성
- 각 단어를 벡터화 하여 셀프 어텐션에 집어넣으면, 한 단어의 점수가 다른 단어들의 위치와 존재 여부까지 고려하여 계산된다
- N개의 단어를 처리할 N*N 패러미터가 필요하므로 메모리 점유율 측면의 한계가 존재한다
- 멀티 헤디드 어텐션 - 예를들어 100차원 벡터가 있고, 10개의 어텐션이 있다면, 벡터를 10등분하여 처리
- NLP뿐만 아니라 최근 들어 Vision에도 사용하고 있음

## Generative Models

- 생성 모델
- 생성 모델을 학습한다? → 단순 생성 뿐만 아니라 구분, Unsupervised representation learning도
- 베르누이 분포, categorical distribution 등의 확률분포 응용 필요
- 예를 들어, 이미지는 막대한 양의 패러미터가 있는데, 이를 활용한 확률분포 방법론 중 각 픽셀이 독립적이라 가정했을때의 방법도 있다 → 말도안됨
- 이를 보완하기 위한 조건부 독립확률 - 연쇄 법칙과 베이즈 정리 활용
- Markov assumption 등을 활용하면 조건부 독립확률에 들어가는 패러미터 수를 크게 줄일 수 있음
- 이렇게 어느정도 패러미터 양을 조절해 스윗 스팟을 찾는게 목표
- 이런 방법론을 활용한 것이 Auto-regressive Model
- 이전 정보를 몇 스텝까지 참조하느냐, 데이터의 순서를 어떻게 정하느냐에 따라 성능이 달라짐
- NADE - 이전의 모든 정보를 참조하는 ARM, 인풋의 확률밀도를 계산 가능
- Pixel RNN - 위와 달리 RNN을 활용
- Latent Variable Models
- Autoencoder는 생성모델인가? → 엄밀히는 X
- Variational Inference - variational distribution을 최적화
- ELBO를 최대화 함으로써 학습
- Adversarial Auto-encoder - 정규분포 이외의 분포도 쓰고 싶음, latent distribution을 맞추는 것
- GAN -  Generator, Discriminator가 서로 성장하면서 사실적인 데이터를 창출하는 모델

## Polar Coordinate

- Polar Plot
    - 극좌표계를 사용
    - 회전과 주기성 표현에 적합
    - 간단한 코사인 계산으로 x,y 좌표계로 변환 가능
- Radar Plot
    - 극좌표계를 사용하는 대표적 차트
    - star plot이라 표현하기도 함
    - 데이터의 퀄리티 표현에 좋음
    - 캐릭터 스테이터스 등으로 자주 보임
    - 각 feature들이 독립적이며, 척도가 같아야 함에 유의
    - 면적은 중요하지 않음
    - feature 수가 많을 수록 가독성이 떨어짐

## Pie Chart

- 원을 부채꼴로 분할하여 표현하는 차트
- 많이 보임
- 연구단계에선 지양
- 단독보다는 다른 차트들과 함께 쓰는 것이 좋음
- Donut Chart → 가운데가 빔, 인포그래픽이나 스토리텔링등에 유용
- Sunburst Chart → 계층 데이터 표현에 좋음, 구현 난이도는 쉬우나 가독성이 떨어짐

## 다양한 시각화 라이브러리

- Missingno
    - 결측치를(missing value) 시각화
    - 정렬을 활용해서 null 정보에 대한 분포 확인
- Treemap
    - 계층적 데이터를 직사각형을 이용해 포함관계를 표현 하는 시각화 방법
    - 분할하는 타일링 알고리즘에 따라 형태가 다양해짐
    - 모자이크 플롯과도 유사
- Waffle Chart
    - 와플 형태로 이산적으로 값을 표현
    - icon을 사용하여 pictogram chart로 활용할 수 있음
- Venn
    - 벤 다이어그램
    - 출판 및 프레젠테이션등에 애용
    - 그리 추천하지 않는 방식 → 가독성이 떨어지기 쉬움

# 💻과제 해결과정

## 기본과제

- 기본 과제 1 (MLP)
    - MLP 실습 강의의 코드를 그대로 재현
    - 첫번째 레이어는 인풋에 `xdim`, 아웃풋에 `hdim`
    - 두번째 레이어는 인풋에 `hdim`, 아웃풋에 `ydim` 으로 사이즈 지정
    - `model_pred`는 결과물을 원하므로, `model` 내에 패러미터로 `view`로 벡터화 한 `batch_in` 이미지를 넣는다, `to(device)`로 넣어주는것 잊지 말 것
    - `n_correct`는 타겟과 예측값의 일치 수를 카운팅 해야 한다
    - `optm.zero_grad()`로 `gradient`를 리셋
    - `loss_out.backward()` 역전파 수행
    - `optm.step()`로 optimizer 갱신
- 기본 과제 2 (Optimization)
    - Optimization 실습 강의의 코드를 그대로 재현
    - 레이어의 인풋에 `prev_hdim`, 아웃풋에 `hdim`을 입력, `bias = True`로 세팅
    - 각 옵티마이저에 이름별로 모델을 넣어준다, `lr = LEARNING_RATE` 설정과 모멘텀의 경우 `momentum = 0.9`로 맞춰주기
- 기본 과제 3 (CNN)
    - CNN 실습 강의의 코드를 그대로 재현
    - 컨볼루션 레이어의 채널과, 커널 사이즈, 스트라이드, 패딩을 잘 대입
    - 레이어의 인풋에 `prev_hdim`, 아웃풋에 `hdim`을 입력, `bias = True`로 세팅
    - `optm.zero_grad()`로 `gradient`를 리셋
    - `loss_out.backward()` 역전파 수행
    - `optm.step()`로 optimizer 갱신
- 기본 과제 4 (LSTM)
    - LSTM 실습 강의의 코드를 그대로 재현
    - 포워드 함수 내에 따로 Hidden state와 Cell state를 만들어 줘야 한다
    - 각각의 state에 `n_layer`, `x.size(0)`, `hdim` 을 디멘션값으로 넣어준다
    - out gate 값은 `rnn_out[:, -1 :]`으로 슬라이싱한 텐서를 선형 모델에 넣어준다
- 기본 과제 5 (Multi-head Attention)
    - MHA 실습 강의의 코드를 그대로 재현
    - 단일 Attention의 점수를 코딩할 때 $\left( \frac{QK^T}{\sqrt{d_K}} \right)$ 식을 코드로 옮긴다
    - 멀티 헤드의 점수도 동일 원리로 코딩하나, `transpose` 대신 `permute`를 활용한다

## 심화과제

- 심화과제 1(Visual Transformer)
    - 2019년에 나온 Vision + Transformer 논문 구현
    - 이미지 임베딩 → 트랜스포머 인코더 → Classification
    - tqdm - 프로그레스 바 라이브러리
    - einops - 텐서 변형에 유용한 라이브러리
- 심화과제 2(AAE)
    - AAE = VAE + GAN
    - VAE는 가우시안 분포 외엔 적용이 힘들다는 문제점이 있음
    - 분포를 모르더라도 잘 배울 수 있는 것

# 📄피어세션 정리

- 월, 화요일엔 주로 강의와 퀴즈 풀이 위주의 의논을 하며 피어세션을 보냈다
- 수요일엔 그날 있었던 멘토링에 대해 의논하였고 멘토님이 제시한 NLP 관련 면접 질문들을 하나씩 골라 금요일까지 조사하고 발표하는 시간을 가지기로 하였다
- 목요일엔 TMI 타임을 가지며 최근에 새로 가진 취미나 놀잇거리 등에 대한 썰을 풀었다
- 금요일엔 수요일에 약속했던 면접질문 솔루션 발표회를 가져 모든 멤버가 성공적으로 발표를 마쳤다

# 🤔학습 회고

- 내가 생각하는 활동점수(100점만점)
    - 90점
- 잘했던것, 좋았던것, 계속할것
    - 대부분의 일을 밀리지 않고 처리한 것, 윤리필터 어느정도 가동해서 실언 하지 않은 것, 노트정리 꼬박꼬박 해서 오늘 할 개인 회고록 추가분량 줄인 것
- 잘못했던것, 아쉬운것, 부족한것 -> 개선방향
    - 7시에 일어나야 했는데 자꾸 7시 30분쯤에서야 일어날 수 있었던 것 → 수면패턴 개선
    - 함께 자라기 책 많이 못 읽은 것 → 주말은 독서의 날
- 도전할 것, 시도할 것
    - 함께 자라기 완독 1회, 논문 대충이라도 하나 보고 요약하기, 선대/확률론 제발 좀 추가로 듣기, 깃허브 개인 프로젝트들 리드미 꾸미기.
- 키워드(공부한 것,알게된 것, 느낀 점)
    - Attention, Transformer의 대략적 개념, GAN을 비롯한 Generative model의 대략적인 개요와 역사적 타임라인, 어차피 다들 힘든건 마찬가지니 힘내자.