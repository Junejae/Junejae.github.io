---
layout: post
title: "[부스트캠프 AI Tech 3기] 9주차 정리 & 회고"
categories:
  - BoostCamp
tags:
  - 부스트캠프
  - AI
  - Deep Learning
  - NLP
  - Transformer
---
![Untitled](/assets/img/AITech로고.png)

# ✏️학습정리

## Transformer

- RNN이나 CNN을 쓰지 않고 Attention만으로 구성하는 모델
- 고전적인 구조에서 Attention은 이전 정보를 rnn에 계속 통과시키기에, 그래디언트 문제가 필연적으로 발생한다.
- Bi-directional rnn으로 양방향의 Attention 정보를 concat 하여 히든 스테이트로 나타내는 방식도 연구됐었다.
- Transformer는 한 word/token 벡터를 서로 다른 3가지 가중치 행렬 W_Q, W_K, W_V와의 내적으로 서로 다른 역할을 하게 되는 Query, Key, Value 벡터를 생성하여, 각 word/token 별로 유동적인 Attention을 지니게 한다. 이를 Self-attention 방식이라 한다.
- 이러한 방식은 기존에 RNN 방식이 가지던 Long-Term Dependency 문제를 해결했다 평가받는다
- word/token 별 벡터를 한데 모아 행렬로 만들어 이 모든 연산을 가중치 행렬과의 내적 등으로 사실상의 병렬연산을 진행 → 이 때문에 학습 속도가 빠르다
- Scaled Dot-product attention: Q X K^T 연산은 두 행렬의 차원이 커질수록 분산치도 따라서 커진다. → 이를 softmax시에 큰 값에 가중치가 몰리는 현상이 발생한다. → 이를 해결하기 위해 softmax 전에 결과물을 차원 크기의 제곱근 값으로 나눠줌으로써 안정화 시킨다.
- Multi-head Attention: 동일한 Q, K, V 벡터에 대해 가중치 행렬값이 서로 다른 버전의 Attention을 적용, 결과물을 concat하는 방식
    - 주어진 문장에서 여러 정보를 뽑아낼 필요가 있음 → 각각의 헤드가 서로 다른 종류의 정보를 병렬적으로 뽑아내 취합한다.
- Self-attention은 rnn과 비교할때 더 많은 메모리를 먹는다 → 대신 병렬계산으로 시간복잡도가 사실상 O(1)이 된다.
- residual connection을 멀티 헤드 연산 이후에 연결하여 학습양상을 안정화
- Batch Normalization과 비슷한 컨셉의 Layer Normalization을 하여 이 또한 학습양상을 안정화
- 이렇게 나온 벡터를 Fully Connected에 통과 → residual과 layer norm을 또 수행하는 것이 Transformer의 전체적인 self-attention 모듈 블럭.
- Positional Encoding:
    - Self-attention은 word/token의 나열을 하나의 set으로 인식하게 되어 어순에 따른 민감성이 없는 것이나 다름없게 됨
    - 이를 해결하기 위하여, 인풋 벡터에 포지션별로 다른 값을 가지는 벡터(sin, cos 함수 사용)를 더하는 전처리를 행함
- 더 원활한 학습을 위하여 LR Scheduler를 사용
- 그렇다면, Decoder는?
    - 해답이 되는 output을 디코더에 넣는다. 초기 과정은 인코더의 multi-head attention과 유사하다
        - masked self attention: 학습과정에서 디코더에서 정답 아웃풋을 넣을때, 한 포지션에서 미래의 값을 훔쳐보지 못하게 마스킹 하는 방식
    - 이렇게 나온 결과물 벡터의 query화 + 인코더의 최종 결과물의 key와 value를 이용해 또 다시 Multi-head attention + residual and layer norm 진행.
    - 이 결과물을 FC와 residual and layer norm 수행 → linear and softmax
    - 이렇게 나온 최종 결과물은 한 언어 벡터의 확률분포가 됨 → 이 값을 기반으로 loss 계산과 역전파 수행으로 학습을 한다.

## Self-supervised Pre-training Models

- 최근 동향
    - 트랜스포머와 셀프어텐션 구조는 NLP 뿐만 아닌 다른 분야에도 점점 일반적으로 사용
    - 많은 수의 트랜스포머를 self-supervised learning framework에서 사용하는 추세, transfer learning도 반필수
    - 자연어 생성 분야에선 셀프어텐션 모델은 여전히 단어들의 그리디 디코딩을 수행
- GPT-1
    - 다양한 스페셜 토큰을 제안
    - 12개의 디코더 트랜스포머를 사용
    - 다음 단어 예측 뿐만 아니라, classifier 예측도 수행시킬 수 있음
    - 주제 분류, 논리검증, 유사도 측정, 다중선택 예측 등의 다양한 작업을 모델의 큰 변형 없이 학습 가능하도록 가이드라인 제시
- BERT
    - 현재까지 가장 많이 쓰이는 기학습 모델
    - ElMo의 lstm을 전부 트랜스포머로 치환함
    - Masked Language Model
        - 문장에서 이전 정보만을 토대로 미래 단어를 예측하는 것보단, bi-directional한 접근법이 실제 용례에도 맞다는 컨셉에서 출발
        - 문장내에서 일정 비율의 단어를 마스킹 처리해, 이를 예측하는 학습을 수행
        - 비율은 하이퍼 파라미터: 너무 적으면 훈련비용 상승, 너무 많으면 문맥파악 힘듬, 15%가 이상적이라는 보고
        - 전이학습의 fine-tuning 도중엔 이러한 마스킹 작업이 인풋에 없어서 문제가 될 수 있다 → 15% 정도의 단어는 예측을 하게 하되, 그 중 80%만 마스킹, 10%는 랜덤단어 대체, 10%는 변화 없음으로 세팅함으로써 이 문제를 해결
    - Next Sentence Prediction
        - 두 문장으로 구성된 인풋의 첫 인덱스에 CLS 토큰을 삽입, 이를 기반으로 문장의 인접성을 판단하는 binary classification 수행
    - 모델 아키텍쳐
        - BERT BASE: 12개 트랜스포머, 768차원 인코딩, 12개의 어텐션 헤드
        - BERT LARGE: 24개 트랜스포머, 1024차원 인코딩, 16개의 어텐션 헤드
        - 인풋
            - 서브워드 레벨의 워드피스 임베딩을 사용
            - 포지션 임베딩도 학습
- GPT-1 vs BERT
    - 학습 데이터 사이즈: BERT > GPT-1
    - 스페셜 토큰: BERT는 GPT보다 한 술 더 떠서 다중 문장 포지션 임베딩까지 수행
    - 배치 사이즈: BERT> GPT-1
    - fine-tuning: GPT-1은 모든 부류의 전이학습에 동일한 5e-5 lr 사용을 제안, BERT는 수행 작업마다의 특화된 lr을 제안
- GPT-2
    - 기존 모델보다 트랜스포머량을 증가
    - 더 많은 텍스트로 학습
    - 학습데이터의 질에 집중
    - 대부분의 nlp task를 질의응답의 형태로 학습시킬 수 있다는 아이디어에서 motivation을 얻음
    - 소셜미디어에서 좋아요 수를 기반으로 좋은 글 여부를 판단해 데이터셋 구성
    - transformer 내의 상위레이어로 갈 수록 초기값이 작아지도록 구성
- GPT-3
    - 모델의 파라미터 사이즈를 이전과 비교도 안 될 정도로 크게 증대
    - 막대한 양의 배치 사이즈를 통해 학습
    - fine tuning 없이 zero-shot, one-shot, few-shot learning 효율이 무시무시하게 증가
    - 모델의 사이즈가 클 수록 학습효과가 어느정도 더 좋아짐을 보임
- ALBERT
    - 경량화된 BERT
    - 기존 고성능 NLP 모델의 한계인 큰 메모리 요구량과 늘어지는 학습속도를 해결하려 함
    - Factorized Embedding Parameterization을 통해 임베딩 과정에서의 패러미터량을 축소
    - Cross-layer Parameter Sharing: 멀티 헤드 어텐션에서 각 어텐션별로 따로 계산되던 선형변환 레이어 등을 공유하는 것 → 파라미터 수는 크게 줄어들지만, 성능하락폭은 적음
    - Sentence Order Prediction: 기존 BERT의 Next Sentence Prediction이 별 효용성을 가지지 못하는 것을 개선하고자, 연속된 문장의 순서를 바꿔서 제시하는 방식으로 모델에 더 좋은 학습을 제안
- ELECTRA
    - 마스킹된 단어를 임의로 복원하는 Generator와 이 결과물의 인공성을 파악하는 Discriminator의 구조
    - 두 모델이 적대적 관계로 학습, GAN과 유사한 아이디어
    - Generator는 BERT와 흡사한 모델, Discriminator는 GPT와 흡사한 모델
    - 이렇게 학습을 진행 한 모델 중 Discriminator(ELECTRA)를 전이학습으로 활용
    - 학습량 대비 성능이 크게 나옴
- 모델의 경량화 연구
    - 기존 모델의 성능을 최대한 유지한 채로, 학습속도를 빠르게, 모델 사이즈를 작게 하려는 시도
    - DistillBERT: 큰 Teacher 모델이 내는 결과를 작은 Student 모델로 모사할 수 있도록 학습을 진행(Knowledge Distillation)
    - TinyBERT: Knowledge Distillation을 위와 비슷하게 사용, 다만 Teacher 모델의 중간 결과물들까지 Student가 학습하도록 진행
- BERT는 주어진 문장 밖에 있는 정보(상식 등)를 제대로 활용할 수 없음
    - 전통적 NLP의 Knowledge Graph를 활용하는 것이 방안이 될 수 있음


# 📄피어세션 정리

- 월: 강의
- 화: Github 특강
- 수: 강의
- 목: 오피스아워, 멘토링
- 금: 스페셜피어, 마스터 클래스

# 🤔학습 회고

- 잘했던것, 좋았던것, 계속할것
    - 깃허브 특강 끝까지 다 따라간 것, 코테 공부 많이 한 것, 이리저리 물어보는 자세 확립한 것
- 잘못했던것, 아쉬운것, 부족한것 -> 개선방향
    - 피드백 정리가 제대로 안 되었다 → 실시간으로 적을만한 접근성 있는 매체 필요
- 도전할 것, 시도할 것
    - 허깅페이스, 관계추출 조사하기, 다음주 발표할 논문 읽고 조사하기
- 키워드(공부한 것,알게된 것, 느낀 점)
    - 이상적인 git 사용법, Transformer 기반 언어 모델