---
layout: post
title: "[부스트캠프 AI Tech 3기] 11주차 정리 & 회고"
categories:
  - BoostCamp
tags:
  - 부스트캠프
  - AI
  - Deep Learning
  - BERT
  - GPT
  - NLP
  - KLUE
  - Transformer
  - RoBERTa
---
![Untitled](/assets/img/AITech로고.png)

# ✏️학습정리

## GPT 언어모델

- 
- 트랜스포머의 디코더 부분을 사용한 모델, Bert와는 정반대
- GPT-1의 경우 자연어 문장 분류에서 특화되었었음
- 기학습 언어 모델의 새 지평을 연 것으로 평가됨
- 여전히 Supervised learning을 필요로 하며, labeling에 드는 자원을 감당하기 힘듬
- 지도 학습의 목적함수 == 비지도학습의 목적함수 (언어 Task 한정)
- 따라서 아주 큰 데이터셋을 학습 → 자연어 Task를 학습 가능함을 발견
- 이것을 기반한 Zero, One, Few-shot learning 을 통해 다각적인 Task를 fine-tuning 없이 수행하도록 설정.
- GPT는 가중치 업데이트가 없기에 새로운 지식에 대한 업데이트에 취약하다
- GPT는 글로만 세상을 배우게 되기에 다각적으로 학습하는 방법론을 연구할 필요가 있다.

## 최신 자연어처리 연구

- XLNet: 기존 LM의 문장 내 관계 방향성 문제를 해결하기 위해 나온 모델, 순열 조합 문장 학습과, 토큰 간 거리를 절대값으로 설정하여 해결
- RoBERTa: 기존 BERT 구조에서 학습을 더 어렵게 만들어서 모델 성능을 증가
- BART: Encoder(BERT) + Decoder(GPT), 학습 방법도 기존의 mask 방식보다 더 다양하게 시행
- T-5: Encoder + Decoder, masking을 한 단어가 아닌 의미를 지닌 어절 단위로 시행, 강의시점 기준 SOTA
- Meena: 트랜스포머 인코더 블럭 1개 + 여러개의 디코더 블럭, 챗봇 최적화, 새로운 챗봇 평가 metric인 SSA를 제시
- Controllable LM: PPLM(Plug and Play Language Model) 등이 제안하는 방식, 원하는 단어들이 나오도록 취사선택 가능한 방법론을 도입, 감정을 컨트롤하여 생성 가능
- 편향적인 언어를 학습에서 배제한다는 것이 좋은 언어모델로 직결되지 않는다.
- Multi-modal Language Model
    - 인간은 텍스트 만으로 언어개념을 이해하는 것이 아니다 → Multi-modal 연구를 필수로 해야 한다.
    - LXMERT: 이미지, 자연어를 동시학습, 이미지와 자연어간의 관계 추론
    - ViLBERT: 이미지와 자연어 임베딩을 같이 하여 CLS 토큰에 두 개념의 정보가 혼재되도록 하는 것
    - Dall-e: 자연어로부터 이미지를 생성해내는 모델

# 📄피어세션 정리

- 월: 강의 수강
- 화: 챗봇 오피스 아워
- 수: 뇌 정보처리와 멀티모달 마스터 세션
- 목: 대회 실험, 멘토링
- 금: 대회 실험

# 🤔학습 회고

- 잘했던것, 좋았던것, 계속할것
    - 실험로그 전부 트래킹 한 것, 지속적으로 관점을 바꿔서 생각해본 것
- 잘못했던것, 아쉬운것, 부족한것 -> 개선방향
    - 장기 실험계획을 제대로 못 세운 것 → 밤에 늘어지기 전에 제대로 계획 세우기
- 도전할 것, 시도할 것
    - Text Augmentation의 적극적 활용, Generative 모델을 활용한 semi-supervised learning
- 키워드(공부한 것,알게된 것, 느낀 점)
    - Text Data Augmentation, Pandas 적극적 활용, 여러 EDA 방법