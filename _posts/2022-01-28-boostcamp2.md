---
layout: post
title: "[부스트캠프 AI Tech 3기] 2주차 정리 & 회고"
categories:
  - BoostCamp
tags:
  - 부스트캠프
  - AI
  - Deep Learning
---
![Untitled](/assets/img/AITech로고.png)

# ✏️학습정리

## Intro to PyTorch

- PyTorch: 딥러닝의 기본이 되는 프레임워크 중 하나, 쉬운 코딩 난이도.
- 딥러닝 코드를 밑바닥부터 짜는 경우는 요새 잘 없다.
- 다양한 프레임워크들이 나오지만, 메인이 되기엔 대부분 부족함이 있다
- TMI: 밑바닥부터 시작하는 딥러닝 책 추천
- 2017년 이후론 TensofFlow, PyTorch 등 사용: 많은 자료, 잘 관리 됨, 표준화됨
- 현대 딥러닝 공부는 사실상 프레임워크 공부다
- PyTorch와 TensofFlow를 비교했을때, 둘 다 로우~하이 레벨의 개발은 가능
- 차이점은 그래프 표현법
    - TF - Static(Define & Run), 그래프를 먼저 정의 → 실행시 데이터를 Feed
    - PyTorch - Dynamic(Define by Run), 실행과 동시에 그래프 생성 → 느릴것 같지만 의외로 별 차이 안 나는 경우 많음, DCG(Dynamic Computational Graph)
- 그 외 특징적인 장단점
    - TF - Google 꺼라 프로덕션, 클라우드 등에서 유리
    - PyTorch - 디버그 용이성으로 인해, 논문이나 개발과정에 유리
- 요즘 ML Ops에서도 PyTorch 많이 쓰는 추세, Pythonic 코드 덕에 유리?
- PyTorch = numpy + AutoGrad + Modules
- numpy 구조 → Tensor 객체로 Array  표현
- 자동미분 지원, 다양한 함수들(Dataset, multiGpu, dataAugumentation 등등)

## PyTorch Basics, Operations

- 기초문법은 numpy와 흡사, AutoGrad(자동미분)의 표현이 다소 다름
- 따라서 numpy 공부 → 딥러닝 공부로 직결
- numpy list → Tensor 객체로 다차원 array 처리
- `n_array = np.arange(10).reshape(2,5)` → `t_array = torch.FloatTensor(n_array)`
- `ndim`, `shape` 등의 함수 그대로 사용 가능
- 기본 list 형태 → Tensor 생성 가능, 다만 일반적인 개발 과정에선 이미 있는 모델의 weight값만 다루곤 하고, 생성부터 하나하나 하는 일 잘 없음
- tensor가 저장하는 데이터타입은 기본적으로 numpy 타입과 동일, gpu 사용 여부가 한 가지 차이
- `device(type = 'cuda')` 같은 문법들이 차이
- colab에선 런타임 유형 변경 → GPU 선택하면 됨
    - 이 때 런타임 변경 == 재부팅이기에 주의!
- `view` - `reshape` 랑 비슷한데 조금 다름, 실전에선 `view` 사용 권장
    - view는 메모리 내 원본값 보장, reshape는 보장 안 되고 카피함
- `squeeze`, `unsqueeze` - 차원갯수가 1인 차원 삭제(압축) ↔ 차원갯수 1인 차원 추가
    - Bert에서 자주 씀
- numpy와 문법은 동일하지만 연산법이 다른 것 있음
    - `dot` 은 내적연산, 왠만하면 `mm` 사용 권장
- 보통 텐서의 0번째 데이터는 batch 수를 의미
    - 예를 들어 `torch.rand(5,2,3)#은 5가 batch`
- `torch.nn.Functional as F` 모듈로 다양한 수식 변환 지원
- AutoGrad - PyTorch의 핵심. 자동미분. `backward` 함수 사용.
    - `requires_grad = True`로 세팅 해 줘야 함

## PyTorch 프로젝트 구조

- 평생 Jupyter 에서만 놀 순 없다
- VS Code 등으로 프로젝트 짤 줄 알아야 함
- 코드도 레고블럭 같이 → OOP + 모듈 → 프로젝트
- 다양한 템플릿 존재 → 필요에 따라 수정 하여 사용
- 추천 템플릿
    - github.com/victoresque/pytorch-template
- Colab 원격연결 개발법도 있지만, 자원이 부족한 상황 아닌 이상 잘 안 씀
- 펙토리 패턴 - argument를 넣어 객체를 생성하는 패턴
- `__getitem__(self, idx):` - 인덱스 값을 넣어주면 그 인덱스 내의 값을 불러옴
- config.json을 이용해 새로운 데이터나 코드도 모듈 갈아끼듯 손쉽게 적용, 코드의 수정을 최소화. 구조적으로 효율적.
- Trainer = model, data, loss 등의 하이퍼 패러미터를 넣어주고 `trainer.train()`으로 학습 시작
- 프로그래밍 능력 키우는 것이 중요

## Structures of PyTorch

- 논문 구현을 해 보고 싶을 때 PyTorch로 구현 하려면?
- 논문은 똑같은 구조들의 연속 집합
- 레이어들을 레고블럭처럼 쌓는다 → 딥러닝
- `nn.Module` 이런 구조들을 만들기 위한 함수
- input, output, forward, backward 정의 필요
    - `backward`  - 레이어 안의 패러미터들의 미분 진행

## PyTorch Dataset

- 모델에 데이터를 피딩(feed) 하는 방법
- `transforms` - 데이터 전처리
- `ToTensor` - 데이터 텐서화
- DataLoader: 처리된 데이터를 묶어 모델에 피딩. batch 제작, shuffle 등 기능 함유
- Dataset class: 데이터 입력형태를 정의, 표준화. 데이터 형태에 따라 다름
- Classification 문제는 getitem 시 dict 타입 반환 하는 것을 애용
- 최근에는 데이터셋 별로 표준화된 라이브러리를 씀
- DataLoader class:
    - 데이터의 batch 생성
    - 학습 직전에 데이터 변환
    - Tensor 변환, Batch 처리가 주 업무
    - 병렬처리 고민 → 요즘은 다 잘 되어있음
    - 여러 처리 기법들을 사용할 수 있게 패러미터 존재

## Models of PyTorch

- back bone(이미 학습 된 모델) → fine tuning(transfer learning)이 현재 트렌드
- 학습결과를 공유하고 싶다 → 모델 저장 필요, `[model.save](http://model.save)` 사용,  중간중간 저장(early stopping) 도 필요할 때 있음
- 주요 저장 확장자, `.pt`
- pickle 식으로 저장도
- `torch.summary()` - 모델 구조와 크기를 보여줌. 써먹는걸 권장.
- checkpoint - 중간중간 저장, 모델의 성능을 요약해서 파일명 뒤에 붙여두면 좋음
- transfer learning - 기존의 다른 데이터셋으로 트레이닝 된 모델을 현재 데이터셋에 적용하여 추가 트레이닝. 대용량 데이터로 만든 성능 좋은 모델을 적은 데이터를 추가 학습시켜서 성능 향상을 노림.
    - 현대 딥러닝 학습법의 기초 - 남이 잘 만든 것을 우리가 잘 써보자
    - TorchVision, Hugging Face 등...
    - Freezing - 이미 학습 된 모델의 전체 패러미터를 변경하지 않고, 일부 레이어만 추가학습

## Monitoring Tools for PyTorch

- 딥 러닝 학습시간은 길다 → 그 시간 동안 다양하게 기록하는 것이 중요
- 요즘은 모니터링 도구 애용 : Tensorboard, Weight & Biases
- Tensorboard - TensorFlow 시각화, 딥러닝 시각화의 핵심이자 기본, 다양한 값 저장
- Weight & Biases - DL, ML Ops 중요 도구, 상용화, 기본기능은 무료, 최근 많이 쓰임.

## Multi GPU

- 딥러닝은 데이터 양으로 승부 → 많은 GPU 병렬처리 필요
- 어떻게 멀티 GPU를 다룰 것인가 핵심
- AWS? GCP?
- GPU vs Node - node는 하나의 시스템, 컴퓨터를 지칭
- 예) Single Node Multi GPU  == 한 대의 시스템 내에 많은 GPU
- Nvidia TensorRT → 지원 잘 해줌
- GPU들에 학습 분산하기 → 모델 나누기 or 데이터 나누기
    - 모델 나누기는 Alexnet부터 이미 사용했음
    - 병목, 파이프라인 설계 등으로 인해 쉽지 않음
- 모델 병렬화 - 각 모델을 다른 GPU에 할당
- 데이터 병렬화: 미니 배치 데이터를 병렬적으로 돌려 결과의 평균을 취함.
    - `dataParallel` -평균 취할때 한 GPU만 사용 시 병목 발생 or Global Interpreter Lock
    - `DistributedDataParallel` - 개별 GPU마다 CPU 할당하여 개별적으로 연산의 평균 계산

## 하이퍼 파라미터 튜닝

- 제대로 된 학습 하려면 → 반복 필요 → 끊임없는 튜닝
- 모델 바꾸기, 데이터 변조, 하이퍼파라미터 튜닝 등의 향상법.
- 이 중 하이퍼 파라미터 튜닝이 가장 덜 중요. 요새는 데이터가 더 중요함
- 하이퍼 파라미터 - 학습률, 모델 크기, 옵티마이저 등 사람이 선택 할 수 있는 옵션들
- 기본적으로 Grid & Random 방식의 튜닝
- Grid - HP들을 일정한 단위로 나눠 세팅하고, 그것들을 조합해 규칙적으로 실험
- Random - HP들을 범위 내 무작위 수들로 설정하여 결과를 관측
- Random을 먼저 사용하여 대략적 성능향상 구간을 찾고 → 그 구간에 Grid를 수행하는게 효율적
- Ray 모듈 - ML,DL 병렬처리를 위해 탄생, 기본적으로 분산 병렬 처리 모듈의 표준, HP 서치 지원

## PyTorch Troubleshooting

- Out of Memory 앞으로 자주 보게 될 것
- 왜, 어디서, backtracking은, 로그 파악 어려움
- 가장 기본적 솔루션
    - 배치 사이즈 하향 → GPU 클린 → 재가동
    - 이게 안 되면 코드 잘못 짰을 가능성도 의심
- gpu util 등으로 gpu 자원 사용현황 파악
- `torch.cuda.empty_cache()` 써 보기
    - 사용되지 않는 캐시 정리
    - 메모리 확보
    - `del`은 그냥 관계 끊기라 이거랑 다름
- 트레이닝 중 tensor로 축적되는 변수 확인 - 한 번 밖에 안 쓰이는 것이 계속 메모리에 적층되는 경우 주의
- `del`적절히 사용하기 - Python은 for loop 내부 변수도 loop 이후에 유지, `del`로 버리는게 좋음
- 가능하면 배치 사이즈를 1부터 올려가며 실험하는 것도 좋음
- `torch.no_grad()` 사용하여 backward 과정에서 메모리 적층을 막음

# 💻과제 해결과정

## 기본과제

- 기본 과제 1(Custom Model 제작)
    - 명성이 자자한 부덕이 과제, 모든 퀴즈의 솔루션을 기록하기엔 기초적인 것들이 많아 생략하고, 불지옥 난이도 위주로 기술한다
    - 임의의 크기의 3D tensor에서 대각선 요소 모으기
        - `gather` 함수의 정확한 기능을 알아야 하는 문제
        - 이중 for loop를 사용해도 되지만, 원본 텐서의 사이즈(`C,H,W`)를 먼저 받아와, 대각 요소의 사이즈를 추정하고(`min(H,W)`), 이를 기반으로 `gather`에 입력시킬 `index` 텐서를 제조할 수 있다.
        - 우선 대각 요소의 갯수만큼 `arange`로 일차원 행렬을 만들고, 이를 `view`로 2d화, 이후 `expand`로 3d화 하여 만든 `index` 텐서를 `gather`에 넣어 (`axis = 2`) 나온 결과물을 `view`로 2d화 하면 끝
    - 부덕이 모델 apply - Function 수정하기 (흑마법편)
        - `apply`를 이용해 Parameter b를 추가
            - 이름이 `Function`인 모듈을 찾아 `register_parameter`로 `b`라는 이름의 패러미터 추가
        - `apply`를 이용해 추가된 `b`도 값을 1로 초기화
            - `b.data.fill_(1.)`로 값 초기화
        - `apply`를 이용해 모든 Function을 linear transformation으로 바꾸기 (X @ W + b)
            - `linear_transformation` 함수를 만들고 이 함수 내에 `hook` 함수를 새로 만든다. 이 훅은 `input[0] @ module.W.T + module.b`의 계산을 수행한다.
            - `linear_transformation` 함수는 이름이 `Function`인 모듈을 찾아 `register_forward_hook(hook)`으로 포워드 훅을 등록한다
        - 세 함수 모두 `apply`로 등록하면 끝
- 기본 과제 2(Custom Dataset 및 Custom DataLoader 생성)
    - IrisDataset 완성하기
        - `feature_names`는 문자 그대로 `iris`의 `feature_names` 등록
        - `X`는 일반적으로 `data` 등록
        - `y`는 `target` 등록
        - `len_dataset`는 일반적으로 `X`의 길이를 물어본다
        - `__getitem__(self, idx)` 는 일반적으로 인덱스에 해당되는 `X` 와 `y`의 내부값을 리턴
    - torchvision의 transform를 응용하여 조건에 맞게끔 변환
        - `Resize`, `RandomHorizontalFlip(p=1.0)`, `CenterCrop`을 적절히 사용한다
    - (정형 데이터) Titanic 데이터로 Dataset과 DataLoader 만들어보기
        - `drop_features`에 해당하는 열 먼저 제거
        - `Survived`를 `pop`을 사용해 먼저 `target`으로써 `y`에 집어넣기
        - 이렇게 전처리된 `data`를 `X`에 등록
        - `['Dead', 'Survived']` 따로 등록
        - `data.columns`를 리스트화 하여 `features`에 등록
        - `self.train = train` 넣는 것 잊지 않기
        - `if self.train:
            y = self.y.loc[idx]` 넣어서 학습 중일때만 y 반환
    - (이미지 데이터) MNIST 데이터로 Dataset과 DataLoader 만들어보기
        - `self.path = path` 잊지 말기
        - `X`에 `image`를, `y`에 `label`을 등록
        - 0~9까지의 스트링이 담긴 `classes` 행렬 생성
        - `self.train = train` 넣는 것 잊지 않기
        - `self.transform = transform` 잊지 말기
        - `self._repr_indent = 4` 가 왠지 모르게 빠져 있으니 넣어주기
        - `if self.transform: X = self.transform(X)`
        - `if self.train: y = self.y[idx]` 넣어주기
            
    - (텍스트 데이터) AG_NEWS 데이터를 이용하여 Dataset과 DataLoader 만들어보기
        - 특수문자를 제거하고 모든 알파벳을 소문자화는 전처리 함수 코딩, `re`를 써도 좋고 `str` 내 변형함수를 써도 좋다
        - `X`에 기사 데이터를 넣을 때 `progress_apply`를 이용해 전처리 함수를 적용한다
        - `vocab`은 `X`에서 단어들을 `split` 하여 `set`으로 중복을 제거한 것을 리스트화 한다
        - `encoder`, `decoder`는 `vocab`을 `enumerate` 하여 `dict`화 한다. 서로 키와 값의 순서가 반대임에 주의

## 심화과제

- Transfer learning 실습
    - 교보재로 주는 파일 내에 채워 넣어야 하는 부분에 코멘트가 상세히 되어 있으므로, 차근차근 따라나가면 무리 없이 코딩이 가능하다

# 📄피어세션 정리

- 과제가 많고 난이도도 만만치 않았던 주간이었다
- 피어세션은 주로 과제와 학습에서 몰랐거나 잊고 있었던 항목들을 탐구하는 시간을 가졌다
- 목, 금요일 피어세션엔 어느정도 시간이 남아서 간단한 TMI 세션을 가져 친목을 도모했다

# 🤔학습 회고

- 매일 7시에 일어나기 어느정도 성공, 강의 노트 정리 꽤 상세하게 했음
- 매일 코테 공부 안했음 → 적어도 슬랙채널에 올라온건 다 할 수 있게...
- 뇌에 윤리필터 안 거치고 말함 → 윤리필터 수급&장착
- 심화과제 진이 다 빠져서 조기에 포기 → 포기하면 미래가 고통스러워
- 자기소개 채널에 글 쓰는거 분명 월요일에 하겠다 했는데 어제 겨우 함 → 일처리좀 제때제때 하기
- 도전할 것: 일일 코테 공부, 개발블로그 좀 더 까리하게 꾸미기, 슬랙 추가기능 더 연구해서 뽕 뽑아먹기, 설 연휴동안 선대, 확률론 더 듣기, 심화과제 재도전
- 키워드: 기술문서 탐독의 중요성, 귀찮아서 안 하곤 있었지만 주변에서 계속 날아오는 조언들(논문 꾸준히 읽기), PyTorch 전체적인 사용법과 꿀팁들, 부덕이 과제 만드신 분에 대한 존경심, 일생동안 안 될거라 생각했던 매일 7시 기상 계획이 성공한데서 오는 깨달음(내가 아직 한참 더 할 수 있구나)