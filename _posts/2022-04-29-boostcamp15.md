---
layout: post
title: "[부스트캠프 AI Tech 3기] 15주차 정리 & 회고"
categories:
  - BoostCamp
tags:
  - 부스트캠프
  - AI
  - Deep Learning
  - NLP
  - MRC
  - Word Embedding
---

---
![Untitled](/assets/img/AITech로고.png)

# ✏️학습정리

## MRC Intro & Python Basics

- Introduction to MRC
    - MRC(Machine Reading Comprehension)의 개념
        - 기계 독해
        - 주어진 지문을 이해하고, 주어진 질의의 답변을 추론하는 문제
        - 실제론 검색을 통해 질문관련 지문을 찾아서 그것을 기반하여 대답하는 형식
    - MRC의 종류
        - Extractive Answer Datasets
            - 질의에 대한 답이 항상 주어진 지문의 segment(or span)으로 존재
        - Descriptive/Narrative Answer Datasets
            - 답이 지문 내에서 추출한 span이 아니라, 질의를 보고 생성 된 sentence의 형태
            - 지문에 답이 없을수도 있다는 동기에서 출발
        - Multiple-choice Datasets
            - 질의에 대한 답을 여러 개의 choice 중 하나로 고르는 형태
    - Challenges in MRC
        - 단어들의 구성이 유사하지는 않지만 동일한 의미인 문장을 이해하는 것
        - 질문에 대한 답변을 찾을 수 없는 경우
        - multi-hop reasoning: 여러 개의 문서를 전부 활용하여야만 답을 찾을 수 있는 경우
    - MRC의 평가 방법
        - Exact Match: 예측한 답 == Ground Truth 의 전체 대비 비율
        - F1 Score: 예측한 답이 Ground Truth와 비슷할 때 partial score를 줌
        - ROUGE-L: 예측한 답과 Ground Truth 사이의 overlap recall
        - BLEU: 예측한 답과  Ground Truth 사이의 precision
- Unicode & Tokenization
    - Unicode: 전 세계의 모든 문자를 일관되게 표현하고 다룰 수 있게 만들어진 문자셋
    - UTF-8: 현재 가장 많이 쓰는 인코딩 방식. 문자 타입에 따라 다른 길이의 바이트를 할당한다.
    - Python에서 Unicode 다루기
        - Python3부터 string 타입은 유니코드 표준을 사용
            - ord(): 문자를 유니코드 code point로 변환
            - chr(): code point를 문자로 변환
    - Unicode와 한국어
        - 한국어는 한자 다음으로 유니코드에서 많은 코드를 차지하고 있는 문자셋
        - 완성형: 현대 한국어의 자모 조합으로 나타낼 수 있는 모든 완성형 한글 11172자(가 ~ 힣)
        - 조합형: 조합하여 글자를 만들 수 있는 초,중,종성
    - 토크나이징
        - 텍스트를 토큰 단위로 나누는 것
        - subword 토크나이징: 자주 쓰는 조합은 한 단위로, 안 쓰이는 조합은 subword로 쪼갠다, BERT에서 사용하는 방식
        - BPE: byte-pair encoding, 데이터 압축용으로 제안된 알고리즘, NLP에서 활발하게 토크나이징 용도로 사용중
- Looking Into the Dataset
    - KorQuAD 살펴보기
        - LG CNS가 AI 언어지능 연구를 위해 공개한 질의응답/기계독해 한국어 데이터셋
        - SQuAD의 데이터 수집 방식을 벤치마크하여 표준성 확보
    - HuggingFace datasets 라이브러리
        - 자연어처리에 사용되는 대부분의 데이터셋과 평가 지표를 접근하고 공유할 수 있게끔 만든 라이브러리

## Extraction-based MRC

- Extraction-based MRC
    - 질문의 답변이 항상 주어진 지문 내에 span으로 존재
    - EM과 F1을 기준으로 평가
    - pre-process → fine-tuning → post-processing의 큰 구조를 지님
- Pre-processing
    - BPE 방법론중 하나인 WordPiece Tokenizer를 주로 사용
    - Attention Mask
        - 입력 시퀀스 중에서 attention을 연산할 때 무시할 토큰을 표시
        - 0은 무시, 1은 연산에 포함
        - 보통 pad 토큰을 제외할때 사용
    - Token Type IDS: 입력이 2개 이상의 시퀀스일때(질문+지문) 각각에 id를 부여하여 모델이 구분하여 해석하도록 유도
    - 모델 출력값
        - 정답은 문서 내에 존재하는 연속된 단어토큰(span), 즉 span의 시작과 끝 위치를 알면 정답을 맞출 수 있음
        - Extraction-based에선 답안 생성보다, 시작과 끝 위치를 예측하도록 학습. → Token Classification 문제로 치환
- Fine-tuning
    - 본 과제에선 주로 BERT를 이용하여 fine tuning
    - output token들은 각각이 답의 시작/끝 토큰일 확률을 내포, 이를 실제 답의 시작/끝 위치와 cross-entropy loss 계산
- Post-processing
    - 불가능한 답 제거
        - end position이 start position보다 앞에 있는 경우
        - 예측한 위치가 문맥 상이 아닐 경우
        - 미리 설정한 max answer length보다 길이가 더 긴 경우
    - 최적의 답안 찾기
        - position prediction 중 score(logits)가 가장 높은 N개를 찾는다
        - 불가능한 조합을 제거
        - 남은 것들을 score의 합이 큰 순서대로 sort
        - score가 가장 큰 조합을 최종 예측으로 선정
        - Top-k가 필요할 시 순서대로

## Generation-based MRC

- Generation-based MAC란
    - extraction-based = 지문 내 답의 위치를 예측 → 분류 문제
    - generation-based = 주어진 지문과 질의를 보고, 답변을 생성 → 생성 문제
    - 평가 방법: extraction과 같은 metric 사용도 가능하지만, BLEU와 ROUGE 스코어로 판별이 우선
    - Seq-to-seq PLM 구조만, classifier는 없음
    - free-form text 형태로 답을 형성
- Pre-processing
    - 입력 표현: input은 extraction-based와 비슷
        - WordPiece Tokenizer를 주로 사용
        - CLS, SEP, PAD 등의 스페셜 토큰 사용
        - attention mask 존재
        - BERT와 달리 BART에선 입력 시퀀스 구분이 없어 token_type_ids가 input에 들어가지 않음
    - 출력 표현: 정답 출력
        - 토큰의 시작과 끝 위치를 출력할 필요가 없음
- Model
    - BART: 트랜스포머 encoder와 decoder 둘 다 존재
- Post-processing
    - 다음 단어를 유추할 때, search 알고리즘을 사용
    - 주로 Beam serach와 유사한 방식인 exhaustive search 사용

## Passage Retrieval - Sparse Embedding

- Intro to Passage Retrieval
    - 질문(query)에 맞는 문서(passage)를 찾는 것
    - MRC와 연결할 시 Open Domain QA 구현 가능
    - query와 passage를 각각 임베딩한 뒤 유사도로 랭킹을 매기고, 유사도가 가장 높은 passage를 반환
- Passage Embedding & Sparse Embedding
    - Passage Embedding Space
        - Passage Embedding의 벡터 공간
        - 벡터화된 Passage를 이용하여 Passage간 유사도를 구할 수 있음
    - Sparse Embedding
        - Bag of Words를 활용하여(n-gram) target term의 문서 내 빈도 등을 측정
        - term이 정확하게 나와있지 않은 경우 곤란해짐
- TF-IDF
    - Term Frequency: 단어의 등장 빈도
    - Inverse Document Frequency: 단어가 제공하는 정보의 양
    - TF 구하기: 해당 문서 내 단어의 등장 빈도 측정, normalize
    - IDF 구하기: term이 등장한 document의 총 document 갯수 대비 비율 측정
    - TF-IDF = TF * IDF
    - BM25: TF-IDF를 바탕으로 문서의 길이까지 고려하여 점수를 매기는 방법

## Passage Retrieval - Dense Embedding

- Dense Embedding
    - TF-IDF 벡터는 벡터의 크기는 크지만 주요 값은 그 내에서 적게 나타난다
    - 무엇보다 이런 sparse 타입의 문제점은 term의 유사성을 제대로 고려할 수 없다는 것
    - Dense Embedding 이란
        - 더 작은 차원의 고밀도 벡터(50~1000)
        - 각 차원이 특정 term에 대응되지 않음
        - 대부분의 요소가 non-zero
    - sparse와 dense 모두 활용하는 케이스도 많음
    - PLM을 활용해 question과 passage를 각각 인코딩 하는 모델을 따로 두고, 두 결과의 유사도를 dot product를 이용해 측정
- Training Dense Encoder
    - PLM의 CLS 토큰을 이용해 Question과 Passage의 유사도를 측정
    - 학습 목표: 연관된 question과 pessage의 dense embedding간의 거리를 좁히는 것, higher similarity
    - negative samples를 이용하여 학습의 질을 높이기도 함
    - 목적함수(objective function): positive passage에 대한 negative log likelihood(NLL) loss 사용
    - metric: top-k retrieval accuracy, retrieve 된 passage 중에 답을 포함하는 passage 비율 측정
- Passage Retrieval with Dense Encoder
    - inference: passage와 query 간의 거리가 가까운 것 순으로 반환
    - 이렇게 retriever를 통해 찾아낸 passage 기준으로 답을 추론
    - 개선법
        - 학습 방법 개선
        - 인코더 모델 개선
        - 데이터 개선

## Scaling up with FAISS

- Passage Retrieval and Similarity Search
    - MIPS(Maximum Inner Product Search)
        - Query와 Passage간의 유사도는 관련성을 측정함으로써 구함
        - 관련성이란 두 벡터간의 내적(Inner Product)의 상대적 크기, 클 수록 관련성 있음
    - 모든 Passage Embedding에 일일히 내적값을 계산하기엔 비효율적
    - Tradeoffs of similarity search
        - 검색 속도 - 보유한 벡터량이 클 수록 오래 걸림 → Pruning
        - 메모리 - 벡터량이 클 수록 전부 다 ram에 올려놓을 수 없다 → Compression
        - 정확도 - 브루트포스와 비교했을때 얼마나 결과가 비슷한지 → Exhaustive Search
        - 더 정확한 검색을 할 수록 오래 걸리게 됨
        - Corpus 크기가 커질수록 탐색 공간이 커져 검색이 어려워지고, 메모리 점유율도 심각해짐
- Approximating Similarity Search
    - Compression - 원래 4바이트 플로트32를 쓰고 있었지만 1바이트로 줄인다
    - Pruning - 검색공간을 인접 군집으로 한정시켜 검색 절대량을 줄임
- FAISS
    - Facebook에서 만든 오픈소스 라이브러리
    - 효율적인 유사도 검색을 위해 사용

## Linking MRC and Retrieval

- Open-domain Question Answering
    - MRC: 지문이 주어진 상황에서 질의응답
    - ODQA: 지문이 따로 주어지지 않음. 방대한 world knowledge에 기반하여 질의응답
    - 구글 등 현대 검색엔진이 ODQA를 활용
- Retriever-Reader Approach
    - Retriever: 데이터베이스에서 관련문서를 검색
    - Reader: 검색한 문서에서 답변을 추출
    - Distant Supervision
        - 질문, 답변만 있는 데이터셋에서 학습 데이터 만들기.
    - inference
        - retriever가 관련련 문서 top k개 출력
        - reader는 k개의 문서를 읽고 답변 예측
        - reader가 예측한 답변 중 가장 score가 높은 것을 최종 답으로 사용
- Issues and Recent Approaches
    - different granularities of text at indexing time
        - passage의 기본 단위를 정해야함
        - top k 개의 정확한 갯수를 정해야함
    - single vs multi-passage training
        - single passage: 각 passage당 독립적으로 답을 찾아낸 후 score 랭킹으로 최종결정
        - multi passage: top k passage 전체를 하나의 passage로 취급, 거기서 하나의 답을 찾도록 함
    - importance of each passage
        - retriever 모델에서 추출된 top k passage들의 retrieval score를 reader 모델에 전달

# 📄피어세션 정리

- 월: 강의
- 화: 강의
- 수: 멘토링, 마스터 클래스
- 목: 최종 프로젝트 관련 멘토링
- 금: 오피스 아워

# 🤔학습 회고

- 잘했던것, 좋았던것, 계속할것
    - 베이스라인 코드 뜯어본 것, 이력서, 삶의지도 보충한 것
- 잘못했던것, 아쉬운것, 부족한것 -> 개선방향
    - 강의는 빨리 들었지만 스페셜 미션 등의 숙제를 하는 데 지지부진했다 → 주말에 다 하자
- 도전할 것, 시도할 것
    - dense retriever, 이력서, 삶의지도 마무리
- 키워드(공부한 것,알게된 것, 느낀 점)
    - ODQA, Passage Retrieval, 기술문서 탐독의 중요성