---
layout: post
title: "[부스트캠프 AI Tech 3기] 6주차 정리 & 회고"
categories:
  - BoostCamp
tags:
  - 부스트캠프
  - AI
  - Deep Learning
  - CV
  - Image Classification
  - 대회
---
![Untitled](/assets/img/AITech로고.png)

# ✏️학습정리

## Competetion with AI Stages!

- P스테이지는 데이터 경진대회를 모티브로 진행
- 실습 위주의 실전 스킬 학습
- 전처리, 학습 추론까지 전체적인 과정을 경험
- 요약하자면 최대한 실전 코드를 많이 쳐 볼 수 있는 기회를 제공
- 이론을 바탕으로 실제 데이터와 코드베이스를 이해
- 실습으로 점진적 모델 성능 향상을 경험
- ML 개발 파이프라인 경험
- Competetion이란
    - 주최자가 주어진 데이터를 가지고 문제를 해결하기 위한 가장 좋은 방법을 공모
    - 가장 중요한 것은 Overview, 보면서 방향성을 잡기
        - 방향성은 상당히 중요하다 - 프로젝트 도중 의사결정에 큰 도움이 된다
    - Overview를 보면서 가장 먼저 설정해야 하는건 Problem Definition, 문제를 정의하는 것
    - Data Description
        - 데이터 스펙 요약본, 필히 체크해야 함
    - Notebook - 여러 과정을 이 서버에서 연습 가능
    - Submission, Leaderboard - 테스트 예측 결과물을 제출하고 순위를 확인
    - Discussion - 등수를 올리는 것 보다, 문제를 해결하려고 하는 마음을 가지고 다른 팀들과 의견을 교류해서 함께 자라나가자
        - 물론 마지막 1-2주간은 엄청 critical한 정보를 공유하진 않는다
    - competetion에서는 데이터 마이닝과 deploy 단계를 제외한 거의 모든 ML 파이프라인을 체험할 수 있다

## Image Classification & EDA

- ML 파이프라인 중 Data Analysis에 대해 싶도깊게 이야기 해 보자
- EDA
    - 탐색적 데이터 분석 - Exploratory Data Analysis
    - 데이터를 이해하기 위한 노력
    - 그런데, 대체 뭘 해야 할까
        - 마치 자유 의견 서술 문제같은 막막함
        - 처음부터 완벽할 정도로 할 순 없는 법이다
        - 과연 내가 EDA를 어렵게 생각하고 있는 게 아닐까 생각해보자
        - EDA의 진짜 목적은 - 뭐든지 좋으니 내가 이 데이터에서 느낀 궁금한 점을 탐색하는 것, 도구는 뭐든지 쓸 수 있다
        - EDA는 거창한 것이 아니다 - 타인의 굉장한 결과물에 매몰되지 말자
        - 한 방에 완벽한 EDA를 수행할 순 없을 것이다 - 결국 다시 해 보게 될 것
        - 처음에는 그냥 아무거나 막 써 보자
- Image Classification
    - Image - 시각적 인식을 표현한 인공물(Artifact)
    - input → model→ output
    - 데이터 형태를 알아야 인풋을 제대로 설정하고, 아웃풋 세팅도 제대로 할 수 있다
    - image → classification model → class
- Baseline
    - Data analysis → data processing → modeling → Training 등으로 순차적으로 제공
    - 직접 보고 이해하며 따로 쳐 보며 구현해 보는 것을 추천

## Dataset

- ML 파이프라인 중 Data Processing 단계에 대해서 논하자
- 주어진 Vanilla Data를 모델이 잘 받아들일 수 있는 형태의 Dataset으로 변형한다
- 전처리 작업 - pre-processing
    - 현업에선 데이터 수집 자체에 어려움이 있다
    - 전처리에 대부분의 시간을 할애할 수도
    - 데이터 사이언스 = 0.8 전처리 + 0.2 나머지
    - 대회용 데이터는 보통 품질이 매우 양호한 편이다
    - 바운딩 박스 - 이미지 내의 꼭 필요한 정보만 크롭
    - 리사이즈 - 계산 효율을 위해 적당한 사이즈로 변경
- 일반화 작업 - generalization
    - bias & variance - 학습이 너무 안 됐거나, 지나치게 많이 됐거나, 오버피팅, 언더피팅을 고려
    - Train/Validation - 트레이닝 데이터셋중 일부를 따로 떼어, 오직 테스트 용으로만 사용
    - data augmentation - 주어진 데이터를 다양하게 변형하여 Case와 State를 여러개 마련한다, `torchvision.transforms`를 이용해 코드 내에서 바로바로 적용 가능하다, `Albumentations`를 따로 쓰면 더 다양하게 변형 가능.
    - 다만, 이런 방법들이 무조건 좋은 것은 아니다.
        - 정의한 문제를 깊이 바라보며, 어떤 방법론을 적용해야 효율적인 다양성을 가질 수 있는지를 가정하고, 실험으로 증명해야 한다.

## Data Generation

- 데이터셋을 잘 구성해도, 그것을 잘 출력해야만 쓸만해진다.
- Data Feeding
    - Generation보다 Feeding이 더 잘 어울리지 않나 싶다
    - 먹이를 대상의 상태를 고려해서 적정한 양을 준다 - Feed의 의미
    - 데이터 제너레이터와 모델 각각의 데이터 처리속도가 제대로 맞물리지 않으면, 퍼포먼스가 떨어지게 된다
    - 이렇듯 요새는 최적화도 현업에서 많이 중요해졌다
    - compose 내에 변형 함수의 배열 방법에 따라 결과물이 같더라도 처리 속도가 달라지는 경우도 있다
- torch.utils.data
    - Datasets - 데이터를 실제로 집어넣게 되는 클래스, 인덱스나 사이즈 등을 기본적으로 호출 가능, Vanilla 데이터를 원하는 형태로 출력
    - DataLoader - 내가 만든 Dataset을 효율적으로 사용할 수 있는 관련 기능을 추가해줌
    - Dataset과 DataLoader의 일은 엄연히 달라서 분리되어 사용되는 것이 좋다.

## Model

- ML 파이프라인 중 Modeling을 알아보자
- 모델이란 시스템을 표현하는 정보적인 방식 - 시스템의 모형
- Pytorch
    - ML 연구, 공부 시작용으로 안성맞춤인 ML 프레임워크
    - Pytorch만의 철학 - 로우레벨, 파이써닉, Flexivility
- nn.Module
    - Pytorch 모델의 레이어는 nn.Module 클래스를 상속한다
    - modules로 한 모델의 세부 모델 정보를 다 살펴볼 수 있다
    - forward - 모델이 호출 되었을때 실행 되는 함수
    - Parameters  - modules가 보유한 계산에 쓰일 패러미터
    - 각 모델은 data, grad, requires_grad 변수 등을 보유
- 파이써닉 하다는 것의 장점 - 응용이 쉬워지고, 에러 핸들링도 편해진다
- CV는 상당히 많이 발전해 있다
- ImageNet이라는 역사적인 데이터셋의 공헌이 큼
- Pretrained Model - 이미 존재하는, 학습이 되어있는 모델
- 미리 학습한 좋은 모델을 바탕으로 내 목적에 맞게 다듬어서 사용하자
- torchvision.models 에 기학습 모델이 많아서, 이걸 사용하면 시간적으로 매우 효율적(pretrained=True)
- 이런 방법론을 Transfer Learning이라 칭한다
- 기존 모델과 내가 설정한 문제와 비교한다, 어느정도의 연관성이 있는지를 파악한다
- Case by Case
    1. 문제를 해결하기 위한 학습 데이터가 충분히 존재한다
        1. 유사성이 높은 모델 - cnn 백본을 프리즈, classifier는 여전히 학습시킨다, feature extraction
        2. 유사성이 낮은 모델 - cnn 백본과 classifier 둘 다 학습시킨다, fine tuning
    2. 학습 데이터가 별로 없다
        1. 유사성이 높은 모델 -  cnn 백본을 프리즈, classifier는 여전히 학습시킨다, feature extraction
        2. 유사성이 낮은 모델 - 포기해라

## Training & Inference

- ML 파이프라인 중 Training 단계에 대하여 논하자
- 학습에 필요한 3대 요소 - Loss, Optimizer, Metric
- Loss - 손실함수, loss도 사실 nn.Modules, loss.backward()가 실행되면 모델의 파라미터의 grad 값이 업데이트, (오차)역전파에 사용.
    - Focal Loss - Class Imbalance 문제가 있을 때, loss치를 밸런스 맞춰 조정해서 부여
    - Label Smoothing Loss - Class target label을 OneHot으로 뱉지 않고 조금 Smoothing 해서 일반화 성능을 향상
- Optimizer
    - 어디로 얼마나 움직이느냐 - 영리하게 움직일수록 수렴이 빨라진다, 학습률을 조정하는게 optimizer
    - LR Scheduler - 학습 시에 학습률을 동적으로 조정
        - StepLR - 특정 Step마다 LR 감소
        - CodineAnnealingLR - 코사인 형태처럼 LR을 급격히 변경, 로컬 미니마 탈출에 용이
        - ReduceLROnPlateau - 더 이상의 성능 향상이 없을 때 LR 감소
        - 결국 방식의 차이고 더 나은건 없다. 취사선택이 중요.
- Metric - 모델의 퍼포먼스를 판단하는 객관적인 지표
    - 모델의 평가
        - 여러가지 평가 지표가 존재함 ( Accuracy, F1-score, precision, recall, RUC-ROC, MAE, MSE, MRR, NDCG, MAP 등등)
        - 여러 대회를 거치며 어떤 metric이 어느 문제에 어울릴지를 가늠하는 실력을 기르자
    - Metric의 허와 실
        - 데이터가 언밸런스하면 정확도에 의구심이 생긴다
        - 데이터 상태에 따라 적절한 Metric을 선택하는 것이 중요
            - 분포 적절 - Accuracy
            - 언밸런스 - F1-Score
- Training Process
    - 준비
        - DataLoader(Dataset) → Model(pretrained)
    - 프로세스 이해
        - model.train() - mode=True 패러미터를 넣어준다. 학습과 테스팅에서 dropout 등이 다르게 작동해야 하기 때문
        - optimizer.zero_grad() - 이전 배치의 grad 영향을 초기화
        - optimizer.step() - 그래디언트 적용을 수행
    - Gradient Accumulation
        - gpu 메모리는 한정적, 배치 사이즈가 중요한 작업이 있을 때 미니 배치로 트레이닝을 여러번 돌리면서 n번마다 optimizer.step()을 콜
- Inference Process
    - model.eval() 하고 진행
    - with torch.no_grad(): - 이 안의 모든 module들은 업데이트가 되지 않을것
    - Validation 확인 - 추론 과정에 Validation 셋이 들어가면 그게 곧 검증
    - Checkpoint 짜서 높은 점수가 나온 것들을 저장. torch.save()
    - 최종 Submission 스펙을 확인 한 후, 변환하여 제출
- Pytorch Lightning
    - 현업에서 매번 이렇게 제로베이스부터 코드를 짜는 경우는 잘 없다
    - 마치 Keras처럼 간단한 코드로 ML을 수행하는 것이 바로 Pytorch Lightning 라이브러리
    - 생산성이 폭증하므로 전망이 좋음
    - 그래도 공부는 Pytorch로 마스터 한 뒤에야 건드리는게 좋다

## Ensemble

- 여러 실험 → 여러 모델로 여러 결과를 형성했을 것이다
- 이 모델들을 버리지 말고 결합해서 더 나은 결과를 도모할 수 있다
- 현업에선 컴퓨팅 자원 문제로 기피하는 편이긴 하다
- Ensemble - 싱글 모델보다 더 나은 성능을 위해 서로 다른 여러 학습 모델을 사용하는 것, 마치 오케스트라처럼
- Model Averaging - Voting, 한 데이터에 대한 모든 모델의 투표결과를 참조하여 결정, Hard or Soft Voting
- Cross Validation - Train set의 일부를 Validation Set으로 쓰면서, 이걸 학습에 활용할 수 있을까 → 모델 별로 쓰는 validation Set을 달리 하고 앙상블
    - Stratified K-Fold Cross Validation - 가능한 경우를 모두 고려 + split 시에 Class 분포까지 고려
    - K값은 보통 5개부터 시작, 너무 많아지면 컴퓨팅 시간에 악영향, 모델 수가 많아지므로
- TTA - Test Time Augmentation, 테스트셋을 변조하면 이 또한 원본과 같은 결과를 출력할까?
    - 테스트 이미지를 Augmentation 후 모델 추론, 출력된 여러가지 결과를 앙상블
- 앙상블은 확실히 효과가 있지만, 그만큼 학습과 추론에 드는 시간이 폭증한다
- Hyperparameter Optimization - 앙상블 만큼이나 시간이 들지만, 효과는 기대하기 힘든 방법
    - Hyperparameter - 시스템 매커니즘에 영향을 주는 주요 패러미터
    - 패러미터를 변경할 때마다 학습을 해야 하니 작업시간 폭증
    - Optuna - 이걸 해 주는 라이브러리, 패러미터 범위를 주고, 범위 내에서 trial 수만큼 실험

## Experiment Toolkits & Tips

- 학습 시각화
- Tensorboard - 학습 과정을 기록하고 트래킹 하는 라이브러리
    - 파이토치에서도 사용 가능하게 됨
    - 학습에 사용된 이미지 샘플도 관람 가능
- Weight and Bias, WandB
    - 딥러닝 로그의 깃허브 같은 존재
    - wandb login - 만든 계정을 써서 로그인
    - init, log 설정
    - 페이지에서 로그 확인
- Jupyter Notebook
    - 코드를 cell단위로 빠르게 실행해볼 수 있음
    - EDA 하기 딱 좋은 툴
    - 학습 도중 창이 꺼지면 되돌릴 수 없음
- Python IDLE
    - 구현은 한번, 사용은 언제나, 간편한 코드 재사용
    - 디버깅 하기 아주 좋음, 특히 파이토치 쓸 때...
    - 실험 핸들링이 자유로움
- Tips
    - 대회에서 분석 코드 보다 설명글을 더 유심히 봐라 - 필자가 생각하고 있는 흐름을 헤아릴 수 있다
    - 코드를 볼 때에는 디테일한 부분까지 분석하자 - 남의 아이디어를 흡수할 수 있다
    - Paper with Codes - 최신 논문과 그 코드까지 볼 수 있는 웹사이트
    - 배움을 서로 공유하자 - 나의 틀린 점을 알 수 있고, 남의 인사이트로 내가 발전할 수 있다

# 🤔학습 회고

- 타임라인
    - 월: 강의듣기, 타운홀 미팅
    - 화: 강의듣기
    - 수: 멘토링, 두런두런
    - 목: 오피스 아워
    - 금: 마스터 클래스
- 내가 생각하는 활동점수(100점만점)
    - 95점
- 잘했던것, 좋았던것, 계속할것
    - 얼떨결에 초반에 상위권에 드는 모델을 짠 것, 최대한 실험적 사고를 유지한 것, cli 베이스 개발환경을 고집한것
- 잘못했던것, 아쉬운것, 부족한것 -> 개선방향
    - 실험할때 변수 통제를 제대로 못 한 것, 모델쪽에 지나치게 집중했던 것, 타 팀과의 소통이 부족했던 것
- 도전할 것, 시도할 것
    - 파이썬 프로젝트 환경에 더 익숙해지기, 스페셜 미션 제대로 해 보기, 더 신뢰성 있는 데이터셋 조성하기
- 키워드(공부한 것,알게된 것, 느낀 점)
    - Feature Extraction, 메모리 폭발, 올바른 실험계획, 점수에 연연하지 말자