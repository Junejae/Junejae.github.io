---
layout: post
title: "[부스트캠프 AI Tech 3기] 8주차 정리 & 회고"
categories:
  - BoostCamp
tags:
  - 부스트캠프
  - AI
  - Deep Learning
  - Attention
  - NLP
  - Tokenization
---
![Untitled](/assets/img/AITech로고.png)

# ✏️학습정리

## Intro to NLP, Bag-of-Words

- NLP = NLU + NLG, Understanding + Generation
- 메이저 학회: ACL, EMNLP, NAACL
- 다양한 Task:
    - 로우 레벨 Parsing: Tokenization, stemming
    - Word & phrase level: NER, PPS, 고유명사 인식
    - Sentence Level: 문맥 분석, 기계번역
    - Multi-sentence & paragraph level: 문장 간 논리 분석, QA, 요약, 대화 시스템
- 텍스트 마이닝: 트렌드 분석 등
- 검색 기술: 상대적으로 발전이 느린 분야, 추천 시스템 등과 연계
- NLP 트렌드
    - 가장 딥러닝에서 연구되고 있는 분야 중 하나
    - Word Embedding: 단어를 벡터화
    - RNN 계열 모델이 핵심화됨
    - Transformer가 나오면서 커다란 향상
    - Self Attention 구조를 많이 쌓아 성능을 올리는 경향 → 범용기술화
    - GPU 자원을 지나치게 많이 요구한다는 문제가 있다
- Bag-of-Words
    - 한 문장에서 각각의 단어를 사전에 넣는다
    - 각 단어를 one-hot-vector화 하여 모두가 동일한 관계를 가진 데이터로 표현
    - 한 문장을 이러한 단어들의 갯수로 표현되는 벡터로 변환 → Bag-of-Words
    - Document Classification에 베이즈 정리가 활용

## Word Embedding

- 단어를 벡터로 표현하는 방법
- 단어간의 관계성을 고려해서 좌표간 거리를 설정하기도 한다: 비슷한 단어가 비슷한 위치에 가도록 → 분류모델이 학습하기 용이해짐
- Word2Vec:
    - 단어간의 관계성을 이용한 알고리즘
    - 기계번역, 감정분석, 이미지 캡셔닝 등에 활용
- GloVe:
    - loss function이 학습 초기에서 단어간의 관계성에 집중하여, 계산속도를 개선

## Recurrent Neural Network and Language Modeling

- 입력벡터와 이전 스텝의 히든스테이트 벡터의 정보를 결합해서 결과물을 재귀적으로 사출하는 구조
- 모든 패러미터가 모든 순간에서 공유됨
- RNN의 타입
    - One to One: 입력과 출력이 하나
    - One to Many: 입력이 하나, 출력이 여럿 → 이미지 캡셔닝, 첫번째 이후의 입력은 모조리 0의 값을 가지는 벡터들
    - Many to One: 입력이 여럿, 출력이 하나 → 감정분석 등에 사용
    - Many to Many: 입력과 출력이 여럿 → 기계번역 등에 사용, 이전 문장이 다 읽혔냐 여부에 따라 번역을 시작하기도, 처음부터 번역을 하기도 하는 다양한 구조가 있음
    - Character-Level Language Model
        - Character 단위로 토크나이징
        - Backpropagation은 역순으로 진행, 한꺼번에 많이 하기엔 물리적으론 무리, 일정량 끊어서 학습하게 됨
        - 기본적으로 Gradient Exploding or Vanishing 현상이 일어나게 되어 개선이 필요함
        - GRU, LSTM이 생겨난 원인

## LSTM and GRU

- Long Short-Term Memory
    - RNN의 Gradient 관련 문제를 해결하려 나옴
    - 이전 타임스텝에서 2가지 값이 인풋으로 들어옴
    - Hidden state, Cell state 벡터
    - Cell state가 추가된 벡터, 완전한 이전정보 함유, Hidden state는 적당히 가공된 벡터
    - 4가지 다른 게이트를 통해 정보를 가공
- Gated Recurrent Unit
    - LSTM의 경량화 버전
    - Cell state 벡터가 Hidden state 벡터와 통합되어 운영
    - 3가지 다른 게이트를 통해 정보를 가공
- 곱셈이 아닌 덧셈형태의 연산이 Backpropagation 도중 발생하여 Gradient 문제를 해결

## Sequence to Sequence with Attention

- Seq2Seq
    - RNN중 Many to Many 모델에 해당
    - 단어 시퀀스를 인풋으로 받아 인코딩 한 벡터를 디코딩 하여 만든 단어 시퀀스를 아웃풋으로 냄
    - 인코더와 디코더는 서로 다른 RNN을 쓰고, LSTM 구조를 씀
    - Voca 상에 <SoS>, <EoS> 토큰을 따로 정의하여 생성의 시작과 종료를 관리
- Attention
    - 고전 RNN은 크기가 정해져 있는 패러미터 탓에 한계가 있었음
    - 단어 각각을 인코딩하면서 나온 hidden state 벡터를 보존하여, 디코더가 이를 참조하여 단어간의 관계를 보존하며 번역하는 구조 → Attention
    - 역전파는 Decoder와 Attention을 타고 Encoder에 도달하는 순서로 진행
    - inference 과정에서 정답을 끼워넣어 주는 Teacher Forcing으로 하면 학습이 빨라지지만, 이 방법을 전혀 사용하지 않는 편이 결과물의 질이 더 좋아진다.
    - Teacher Forcing을 처음에만 쓰는 학습법도 있음
    - Attention을 구하는 방식
        - Dot Product: 인코더, 디코더 벡터 간의 내적
        - General: 내적 사이에 Weight 매트릭스를 추가
        - Concat: General 연산에 비선형 변환과 추가 Weight 연산을 수행
    - Attention이 성능을 크게 올려줌
        - 집중해야 할 정보를 선별해주기 때문
        - 인풋 길이 문제도 해결
        - 학습에서 Gradient Vanishing 문제도 해결
        - 네트워크 스스로 집중해야 하는 위치를 학습하게 됨

## Beam Search and BLEU score

- Beam Search
    - 일반적인 근시안적으로 셀렉션 하는 것을 그리디 디코딩이라 부름
    - 이런 방식은 잘못된 선택을 되감을 수 없다는 문제가 있음
    - 입력문에 대한 출력문의 확률을 다 계산하는게 최선
    - 현실적으로 무리
    - Beam Search는 절충안을 제안
    - 미리 정해놓은 K개의 가짓수를 고려 → 가장 가능성이 높은 것을 택(일반적으로 k는 5~10으로 세팅)
    - 곱확률에 log를 취해 덧셈으로 연산 → score
    - <EoS> 생성시점이 경우에 따라 다르기에, 임시저장하는 방식을 씀
    - 생성된 가설 별로, 포함된 단어 갯수에 따라 확률이 크게 차이날 수 있다 → 단어 갯수로 확률을 나누어 단어당 평균값을 취하면 된다
- BLEU score
    - Precision(정확도), Recall(재현율), F1 score(정확도와 재현율의 조화평균) 등을 문장 길이와 맞은 단어 갯수로 추산 가능
    - 이런 기존 metric은 어순을 신경쓰지 않는다는 치명적 문제점이 있음
    - BiLingual Evaluation Understudy
        - N개의 연속된 단어 시퀀스가 레퍼런스와 얼마나 유사한지를 측정
        - 1~4개의 연속된 단어에 대한 정확도를 계산 → 기하평균을 냄
        - 이런 기하평균 값에 brevity penalty를 곱하여 문장길이를 적게 예측할 수록 패널티를 줌(recall을 고려하는 것)
        - 0~100%의 값을 가지게 됨

# 💻과제 해결과정

## 기본과제

- 기본 과제 1 - Data Preprocessing & Tokenization
    - 1-A): 기본적으로 re 모듈로 간편히 해결 가능한 문제지만, str() 함수 중 replace(), lower(), split() 등을 조합해도 쉽게 해결이 가능하다.
    - 1-B): Counter()와 enumerate() 함수를 활용하면 쉽게 해결이 가능하다.
    - 1-C): 딕셔너리의 특성을 잘 이해하고 있다면 이 또한 쉽게 해결 가능하다.
    - 2-B): spacy를 이용한 tokenizer의 스트링을 불러올땐, .text나 .lemma_같은 attribute를 써야 하는 점을 이해하면 된다.
- 기본 과제 2 - Word-level language modeling with RNN
    - 2): 모델 클래스의 forward 함수의 상세작업을 정의하는 단계. 지금까지의 수업과 프로젝트를 잘 헤쳐왔으면 설명만으로 제대로 구현 가능하다.
    - 3-1): 데이터를 배치 사이즈에 맞게 분할하는 단계. 먼저 자투리 부분을 슬라이싱으로 끊어버린다 → view 함수로 (batch_size, num_sample, sequence_length) 차원으로 맞춰준다 → transpose로 0번과 1번차원을 뒤바꿔 (num_sample, batch_size, sequence_length)의 형태로 재정립한다.
    - 3-2): inference 코드를 구현하는 단계. training 코드에서 학습하는 부분만 버리고 복붙하면 된다.

# 📄피어세션 정리

- 월: 타운홀 미팅 이후 그라운드 룰을 새로이 정하였다.
- 화: 이번 주 학습 분량 관련 질문 응답과, Further question 탐색, 과제 팁 등을 공유하였다
- 수: 휴강
- 목: Further Question을 좀 더 파고들었다.
- 금: 첫 논문 스터디를 하였다. BertSum에 관한 논문이었다.

# 🤔학습 회고

- 잘했던것, 좋았던것, 계속할것
    - 자력으로 기본과제 1~2 끝낸 것, 대회 이전의 학습루틴으로 회귀한 것
- 잘못했던것, 아쉬운것, 부족한것 -> 개선방향
    - 일기 퀄리티가 영 부실한 것, 결국 금요일까지 제대로 읽은 논문이 없는 것 →  실시간 일기 작성, 논문을 소설읽듯 하는 습관 기르기
- 도전할 것, 시도할 것
    - 깃허브 레포 리드미 정리, 멘토님 피드백 기반으로 대회 회고 보완
- 키워드(공부한 것,알게된 것, 느낀 점)
    - 메타인식, Attention, 스코어링 방법론, 확률론 좀 해야겠다