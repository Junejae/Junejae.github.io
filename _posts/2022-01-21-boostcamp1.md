---
layout: post
title: "[부스트캠프 AI Tech 3기] 1주차 정리 & 회고"
categories:
  - BoostCamp
tags:
  - 부스트캠프
  - AI
  - Deep Learning
  - Pytorch
---

---
![Untitled](/assets/img/AITech로고.png)

# ✏️학습정리

# 파이썬

## 코딩환경

- 윈도우 환경에서 권장되는 커맨드라인 환경은 Anaconda, cmder, Ubuntu for Windows
- 이들 중 Anaconda는 리눅스가 아닌 윈도우의 명령어를 사용해야 한다.

## Jupyter

- 웹 기반 파이썬 Shell과 Text editor의 융합체
- Shell명령어 리스트:
    - conda install jupyter : 설치
    - jupyter notebook	  : 실행(현재 폴더에서)
- 편의성 위주의 다양한 단축기들이 존재
- Google Colab은 Jupyter의 클라우드 기반 변형, 단축키 미묘하게 다름

## 변수

- 파이썬은 '동적 타이핑 언어다': 실행 시점에 데이터 타입을 결정한다
- float(var), int(var) 등의 함수 사용으로 데이터 타입의 변환 가능
- 주의: float -> int 변환시 소수점 이하는 반드시 내림
- str <-> float, int 변환시 연산 실수 주의!(ex: string concatenation)

## 리스트

- 파이썬의 리스트엔 다양한 데이터타입이 한 리스트에 들어갈 수 있다
- 리스트 변수에 저장 되는 것은 리스트 주소값 (ex: my = [1,2,3]에서 my는 메모리 주소값을 저장했음)
- 리스트 슬라이싱:
my_list[:]		: 처음부터 끝까지
my_list[0:9]	: 인덱스 0부터 8까지, 총 9개
my_list[:9]		: 인덱스 0(처음)부터 8까지
my_list[1:9]	: 인덱스 1부터 8까지, 총 9-1=8개
my_list[-63:63]	: 범위 오버시, 알아서 최대범위 지정
my_list[::2]	: 2칸씩 띄어서 픽업 ([::1]이 디폴트)
my_list[::-1]	: 역순으로 픽업
- 리스트 함수들:
my_list1 + my_list2	: 두 리스트의 병합물을 출력
len(my_list)		: 리스트의 사이즈(원소 갯수)
my_list[n] = 'foo'	: 리스트 내 n번째 값을 변경
my_list * 2			: my_list 2회 반복
'cat' in my_list	: 'cat'이 my_list 안에 존재하는지 여부
- my_list.함수:
append, extend, insert, remove
- del my_list[n]	: n번째 주소에 있는 리스트 객체의 삭제 명령
- 패킹&언패킹:
point = [1, 3]	: 1, 3을 변수 point에 패킹한다
x, y = point	: point의 값 1, 3을 변수 x, y에 언패킹한다
- 2차원 이상의 리스트를 구성할 때 *(곱하기) 사용 금지. Manual하게 초기화한다.

## 함수 1

- input() : 콘솔창 '문자열' 입력받는 함수
- 숫자를 입력하더라도 문자열로 들어가니, 활용에 주의 요망
- print('asdf' , 'qwer') : 콤마 활용시 한 칸 띄워주고 문자열 연속 출력
- fstring:
print(f"The result is : {result_data}")
Padding(WIP)

## 조건&반복문

- if,elif,else 세트 사용 시 미처 생각하지 못한 케이스가 있는지 면밀히 체크
- for loop:
for x in y: 			y 내부의 원소를 하나하나 iterate
for x in range(0, 10): 	0부터 9까지 총 10개의 숫자를 순차적으로 iterate
range(0,10) == range(10)
range(0,10,2)			2칸씩 띄어서 iterate(1칸이 디폴트)
range(0,10,-1)			역순으로 iterate
- break : 해당 조건시 반복문 탈출
- continue: 해당 조건시 이번 사이클 내에 남은 명령 건너뛰기
- while은 사용자의 선택에 따른 가변 반복문 구현에 유리하다

## 디버깅

- indentation, 오탈자, case sensitive 이슈들: 초보적으로 일어나는 실수
- 어느정도의 원인은 인터프리터가 잡아준다
- 논리 에러: 매 스탭마다 프린트 문을 작성하여 순차적으로 검증
- if **name** == '**main**': 구문이 존재할 경우 파이썬 Shell에서 실행되지 않으니 적극 활용
- 어지간하면 구글신과 stack overflow가 해결해 주신다

## 문자열

- 1byte = 8bit = 2^8 = 256
- 알파벳 1글자는 1byte
- 문자열은 기본적으로 캐릭터의 리스트, 리스트의 인덱싱, 슬라이싱 기법을 사용할 수 있다
- 문자열 함수:
(WIP)
- 따옴표 3회 연속 사용으로 여러 줄로 구성된 문자열 저장 가능:
예시: s =	''' first
second. '''
- raw_string = r"first \n second" : r을 붙여 \의 특수기능 무시하고 그대로 출력

## 함수 2

- 파이썬 함수는 Call by Object Reference
- 객체의 주소가 바로 함수에 전달되는 방식으로, 동일 이름의 새 객체를 선언하기 전 까진 전달받은 객체가 그대로 사용, 변조된다
- 함수 내에서 글로벌 변수 사용시엔 global var_name 으로 이미 쓰고 있는 글로벌 변수명을 다시 지정해서 선언한다
- 재귀함수: 반드시 종결코드부터 구현
- 함수 타입 힌트:
def my_function(my_var : 변수타입) -> 반환값 타입:
- 함수 작성 가이드(WIP)
- 파이썬 코딩 컨벤션(WIP)

## 데이터구조

- 스택: LIFO, append(), pop()
- 큐: FIFO, append(), pop(0)
- 튜플: ()로 선언, 값 변경 불가능, 그 외엔 리스트와 사용법 비슷
- 셋: 집합, 중복 불허, 순서 없음, my_set = set([4,4,3,2,1]) -> {4,3,1,2}
다양한 집합연산 가능(WIP)
- 딕셔너리: 해시 테이블, 키와 값으로 저장, 파이썬 최신 버전은 입력 순으로 정렬됨
다양한 딕셔너리 기능(WIP)
- 콜렉션: 파이썬에 기본적으로 탑재된 데이터구조 확장 지원 모듈
deque: 리스트보다 더 효율적으로 스택과 큐를 구현, 링크드 리스트
OrderedDict: 과거 파이썬 딕셔너리의 무작위 배치를 입력 순 정렬로 바꿔줬었음
defaultdict: 딕셔너리에 넣은 적 없는 키도 일단 디폴트 값으로 반환시켜줌, 워드 카운팅에 유리
Counter: 시퀀스 타입 데이터의 원소들의 갯수를 딕셔너리 형태로 반환시켜줌, 집합 연산도 가능, 워드 카운팅도 완전 가능
namedtuple: 튜플 형태로 데이터구조체를 저장시켜줌, 데이터타입을 미리 지정해서 선언

## 파이썬 스타일 코드

- Split, join: (WIP)
- list comprehension:for, append 조합보다 속도가 일반적으로 빠르기에 많이 사용(WIP)
- enumerate: 리스트 원소에 번호를 붙여서 추출시킴
- zip: 두 리스트의 값을 병렬 순차적으로 추출시킴
- lambda,map,reduce: 익명함수, 사용례(WIP), 최신 파이썬에선 권장하지 않으나 여전히 많이들 씀, (WIP)
- iterable object: 시퀀스형 데이터 구조에서 데이터를 순서대로 추출하는 객체, iter(), next() (WIP)
- generator: iterable object를 이용하여 메모리 친화적 코딩을 도와줌, yield로 한번에 하나의 원소만 메모리에 반환, generator comprehension은 리스트와 달리 []대신 ()로 선언(WIP)
- function passing arguments: 키워드(WIP), 디폴트(WIP), Variable-length(WIP)
- Variable-length asterisk: 함수의 패러미터가 정확히 정해지지 않았을때, 가변성을 표현하기 위해 사용, *args, *kwargs,(WIP)
- *의 용례: (WIP)

## OOP

- 클래스: 붕어빵틀
- 인스턴스: 붕어빵
- 실제 구현 예시: (WIP)
- 상속: (WIP)
- 다형성: (WIP)
- 가시성: (WIP)
- decorator: (WIP)
- 클로저: 함수 내의 함수를 리턴값으로 반환

## 모듈 & 프로젝트

- 모듈: (WIP)
- namespace:(WIP)
- 파이썬 기본모듈: (WIP)
- 패키지: (WIP)
- 프로젝트 가상환경 설정:(WIP)

## 자잘한 팁

- exception: 예상 가능하거나 예상 불가능한 예외 처리, 오류로 인터프리터가 멈추는걸 방지
- exception handling: try: except <Exception Type>: 구문
- 빌트인 에러코드: 맨 마지막엔 보통 Exception as e: print(e)으로 처리(비권장)
- else를 붙이기도 하지만 불편해보임, 예외 발생시 구동 안 하는 코드
- finally: 예외 발생시에도 그대로 구동하는 코드
- if else 구문으로 때워도 되니 팀과의 협의로 결정
- raise: 필요에 따라 강제로 발생시키는 exception
- assert: 조건 불만족시 강제로 발생, 코테 디버그로 써먹기 유용
- 파일: 기본적으로 텍스트와 바이너리 파일로 나눔, 바이너리는 메모장으로 열면 해설 불가
- 파일 코드 예시: (WIP)
- with 구문으로 활용하는게 효율적
- 한줄씩 읽기: 파일이 너무 클 때 효율적
- 파일 인코딩: 윈도우 디폴트는 cp949, 범용인 utf8로 맞춰주는게 효율적
- import os 모듈로 디렉토리를 다룰 수 있음: os.mkdir, os.path
- import shutil 모듈: (WIP)
- import pathlib 모듈: path를 객체로써 핸들링 가능, 굉장히 유용, OS간 발생하는 문제 억제
- 로그파일 생성 예제:(WIP)
- import pickle: 메모리에 올린 객체를 파일로 저장해서 인터프리터가 꺼져도 휘발성 정보를 남길 수 있음, pickle.dump, pickle.load
- logging handling: (WIP)
- import logging: debug, info, warning, error, critical로 로그를 볼 수 있는 사용자 권한을 조절 가능(WIP)
- import configparser : (WIP)
- import argparse: (WIP)
- logging.Formatter: (WIP)

## 외부 데이터 처리

- CSV: 쉼표(,) 탭 공백 등으로 필드를 구분하는 텍스트 파일, 엑셀양식 자료를 자유롭게 쓰기 위한 데이터 타입(WIP)
- 문장 내에도 들어갈 가능성이 있는 쉼표를 전처리할 필요성
- data.go.kr 예제천국
- import csv: CSV 객체 활용하기 좋음
- cp949, utf8 변환 주의
- HTML: 웹에서 데이터를 표시하기 위한 형식, 태그로 요소 표시, 트리 구조로 구성
- html의 규칙을 분석하여 웹 상의 데이터 추출이 가능
- 정규식: regex, 문자열 패턴 정의 공식, 어느정도 규칙성이 있는 문자열 추출 가능(WIP)
- 정규식 연습장 [regexr.com](http://regexr.com/)
- 정규식 메타 문자(WIP)
- import re: search 한개만, findall 전체
- import urllib.request: url 열기 등에 사용하는 모듈
- XML: html과 비슷한, 태그(마크업)로 데이터 구조와 의미를 설명, JSON이 요샌 대체재로 사용
- from bs4 import BeautifulSoup :가장 많이 쓰이는 xml 파서(WIP)
- JSON: 본래 자바스크립트의 데이터 객체 표현 양식이었으나, 간결성, 적은 용량, 쉬운 코드 전환성으로 인해 요즘 많이 쓰이는 형식. 딕셔너리 타입과 상호 호환 가능함.

## Numpy

- 파이썬 과학 계산용 패키지, 선형대수를 파이썬에서 제대로 지원
- 파이썬이 인터프리터 언어기에, 파이썬만으로 구현하면 성능이슈 -> C로 구현
- conda install numpy 로 따로 설치 해 줘야 함
- import numpy as np
- ndarray: np.array(리스트, 타입), 하나의 타입만 넣을 수 있음.
- 파이썬은 -5~256까지의 정수는 메모리에 미리 저장해 놓아서 그걸 불러오니 주의
- numpy는 메모리 할당을 다 새로 한다
- dtype, shape 등의 정보 표시 함수
- 행렬 차원에 따라 명칭 변화: 스칼라, 벡터, 매트릭스, nth of 텐서...
- np.int64처럼 numpy 자체 데이터타입 설정 가능, 타입별 크기에 따라 딥러닝 파라미터 크기도 영향받으니 요주의
- reshape(): 행렬 형태 변형, 원소 총 갯수는 동일, -1을 넣으면 사이즈 기반으로 딱 맞게 조정
- flatten: 다차원 행렬을 1차원 벡터로 변환
- 인덱싱: 리스트와 달리 [0][0]과 [0,0]의 의미가 동일
- 슬라이싱: 리스트와 달리 행과 열 부분을 나눠 떼어낼 수 있음, 매트릭스의 부분집합 추출 가능(ex: [:2,:]), 스텝 기능도 건재
- 어레이 생성 함수: arange, zeros, empty메모리 초기화 안됨, something_like, identity, eye, diag(WIP)
- operation functions: sum, mean, std, something, axis(오퍼레이션 실행시 기준 축 설정)(WIP)
- concatenate: vstack(concatenate axis 0), hstack(concatenate axis 1)
- newaxis: 축 추가
- array operations: 기본적으로 어레이 간 사칙연산 지원
- element-wise operation: 어레이 간 형태가 같을때 일어나는 연산
- dot product: 행렬곱
- transpose: 전치행렬
- broadcasting: 형태가 다른 어레이 간 연산을 지원, 주의해서 써야 함
- %timeit: jupyter 환경에서 코드 성능을 측정 가능
- 비교분석: all, any, 일반적으로 어레이에 비교 오퍼레이터를 그냥 쓰면 불리언 어레이로 반환
- np.where(WIP)
- argmax, argmin: 어레이 내 최대, 최소값의 인덱스 반환, axis 기반으로 연산
- boolean index: array[condition], condition 조건에 맞는 값만 추출해서 재구성 반환
- fancy index: int 어레이를 인덱스값으로 사용해서 값 추출
- data i/o: loadtxt, savetxt(WIP), np.save-pickle로 저장

## Pandas

- python의 엑셀
- numpy와 통합되어 강력하고 편리한 기능들 제공
- 테이블 형태 데이터 처리 및 분석으로 활용
- 테이블 부르는 용어 정리:(WIP)
- conda install pandas
- 실습형 (WIP)
- 테이블 내 데이터 타입은 다양, 기본적으로 2차원 배열
- 실전에선 데이터프레임을 바로 불러오는 경우 거의 없음
- NaN: Not a Number
- loc, iloc은 서로 다르다. loc은 인덱스 이름 그대로 사용.
- 불리언 타입의 시리즈를 생성해 기존 데이터프레임에 붙일 수도 있다
- T(전치), values, index, to_csv, drop
- selection & drop 테크닉(WIP)
- 데이터 프레임 원본은 최대한 변조하지 않으려는 편(inplace=True를 쓰면 변조)
- series operation (WIP)
- lambda, map, apply (WIP)
- 빌트인 함수: describe, unique, sum, isnull, sort_values, corr상관계수, cov공분산, sort,  (WIP)
- 여러 데이터 핸들링 방법
- groupby: sql의 groupby와 유사한 개념(WIP)
- grouped: 기본적으로 제네레이터 형태 (WIP)
- case study(WIP)
- Pivot Table (WIP)
- crosstab (WIP)
- merge: sql에서 보이는 그것, join (WIP)
- concat: (WIP)

# 선형대수

## 벡터

- 열백터, 행벡터, np,array([1,1,1])
- 벡터 원소의 갯수 = 벡터의 차원
- 벡터는 차원 공간 내의 한 점을 의미
- 벡터는 좌표공간에서 원점으로부터의 상대적 위치를 의미, 스칼라곱을 하면 길이만 변함
- 같은 모양의 벡터끼리 덧셈, 뺄셈, 성분곱이 가능(elemental-wise)
- 두 벡터의 덧셈, 뺄셈이란 다른 벡터로부터의 상대적 위상 이동을 의미
- 벡터의 노름(norm)이란 원점에서의 거리, L1(변화량의 절대값)과 L2 노름(유클리드 거리) 등이 있다.예제(WIP)
- 노름 종류에 따라 기하학적 성질이 다름. L1(마름모) L2(원). 머신러닝에선 둘 다 사용
- 두 벡터 사이의 거리 구하기: 벡터의 뺄셈을 이용해 계산, 구한 거리를 이용해 각도도 계산 가능. 제 2 코사인 법칙 사용. 전부 L2노름 사용.
- 벡터의 내적(inner product) (WIP) np.inner
- 벡터의 내적은 어느 벡터를 다른 벡터 기준으로 정사영(orthogonal projection) 시킨 것의 길이를 기준 벡터의 길이만큼 곱한 것. cos()로 구함

## 행렬이란

- 벡터를 원소로 가지는 2D 배열
- numpy에서는 행이 기본 단위(raw-major)
- 벡터는 소문자, 행렬은 대문자로 표현(ex: x1, x2, x3 와 X)
- 인덱스는 행을 먼저 표현(ex: x11, x12, x 등)
- n * m 행렬은 (WIP)
- 전치행렬 XT는 행과 열의 인덱스가 바뀐 것

## 행렬 이해하기

- 벡터 = 공간에서의 한 점
- 행렬 = 공간에서의 여러 점 = 여러 벡터
- 행벡터 xi = i번째 데이터
- 행렬 원소 xij = i번째 데이터의 j번째 변수의 값
- 행렬은 벡터공간에서 사용되는 연산자(operator)로도 이해 가능
- m차원 벡터에 n*m 행렬을 곱해주면 m차원 벡터로 변환(n → m 차원 이동)
- 이런 행렬곱을 통해 패턴추출, 데이터 압축 등이 가능 (linear transform)

## 행렬의 계산

- 행렬끼리 같은 모양을 가지면 성분간 1대1 덧셈, 뺄셈, 곱셈이 가능(elemental-wise)
- 행렬의 스칼라곱도 벡터와 동일
- 행렬의 곱셈은 i번째 행벡터와 j번째 열벡터 사이의 내적을 성분으로 하는 행렬을 형성.

```python
# 예시
x = n*m #행렬
y = m*n #행렬일시

x@y = n*n #계산 성립

#아다리가 안 맞으면 불가능
```

- 행렬에도 내적은 있다. `np.inner()`로  결과물 행렬이 i*j일때 i번째 행벡터와 j번째 행벡터 사이 내적을 성분으로 가지는 행렬을 계산. 단, 이는 수학적 의미의 내적은 아님.

## 역행렬

- $$어떤 행렬의 연산을 거꾸로 되돌리는 행렬, A의 역행렬은 A^-1으로 표기
- n = m일때, 행렬식이 0이 아닐 때만 성립
- 어떤 행렬과 그 행렬 사이의 행렬곱은 항등행렬, I가 된다
- `np.linalg.inv`로 계산
- 역행렬 계산이 안 되는 경우 유사역행렬 = 무어-펜로즈 역행렬 A^+ 사용
- n≥m, n≤m인 경우가 서로 다르다
- `np.linalg.pinv`를 사용해 연립방정식의 해를 구할 수 있음. Ax = b
- 이를 이용하여 데이터를 선형모델로(linear model) 해석하는 선형회귀식을 도출 가능
- sklearn LinearRegression도 비슷하게 가능

# 경사하강법

## 미분

- 변수의 움직임에 따른 함수값의 변화를 측정하는 도구
- `sympy.diff` 로 계산 가능
- 주어진 함수의 주어진 점에서의 접선 기울기를 구하는 방법
- 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 증가/감소하는지 알 수 있게 됨
- 미분값을 더해 함수의 맥시멈을 구하면  경사상승법, 빼서 미니멈을 구하면 경사하강법(gradient decent)

## 경사하강법의 알고리즘

```python
var = init # 시작점
grad = gradient(var) # 미분 계산 함수

while(abs(grad) > eps): # 컴은 정확한 미분값 계산이 안되니 조건 필요
	var = var - lr * grad # lr = 학습률, eps = 종료조건
	grad = gradient(var)
```

- 변수가 벡터라면(WIP)
- 그레디언트 벡터(WIP)

## 실전

- 비선형 모델에서 무어펜로즈 역행렬은 통하지 않는다
- 따라서 선형회귀 계수를 경사하강법으로 구한다
- 아래 식을 손으로 구해보자(WIP)

## 경사하강법 기반 선형회귀 알고리즘

```python

for t in range(T): # T: 학습횟수, lr: 학습률
	error = y - X @ beta
	grad = - transpose(X) @ error
	beta = beta - lr * grad # beta : output
```

- 경사하강법에서는 학습률과 학습횟수가 굉장히 중요하다
- 적절한 학습률과 학습횟수를 선택했을때 convex한 미분가능 함수에 대헤 수렴이 보장
- 비선형회귀 문제의 경우 목적식이 볼록하지 않아 수렴이 항상 보장되지 않음
- 특히 딥러닝의 목적함수는 대부분 convex 하지 않다

## 확률적 경사하강법

- stochastic gradient descent
- 모든 데이터가 아닌 데이터 한개~일부 활용해 업데이트
- non-convex 목적식을 SGD를 통해 최적화 가능
- 데이터 일부로 패러미터 업데이트를 수행하기 때문에, 리소스를 좀 더 효율적으로 활용 가능

# 딥러닝 방법론

## 신경망

- 대표적인 비선형모델
- 분류문제

## 소프트맥스

- 모델의 출력을 확률로 해석 가능하게 변환해 주는 연산
- 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측>> 오버플로우도 방지했다
- 추론시엔 다르다

## 신경망 상세

- 신경망 = 선형모델 + 활성함수(activation function)
- 활성함수는 소프트맥스와 달리 벡터가 아닌 스칼라를 받아서 계산

## 활성함수

- 비선형 함수로, 딥러닝에서 매우 중요
- 쓰지 않으면 딥러닝에 의미가 없음
- sigmoid, tanh 함수는 레거시, 요샌 ReLu를 주로 씀
- 다층 신경망 구조: 요즘 많이 쓰이는 방식
- 층을 여러개 쌓는 이유: 이론상 2층만으로도 쓸 수 있지만 층이 깊을 수록 목적함수 근사에 필요한 뉴런(노드)의 숫자가 줄어서 효율성이 증가함.

## 역전파 알고리즘

- 딥러닝은 역전파를 사용해 각 층의 패러미터를 업데이트해 학습
- 기본 원리는 합성함수 미분법인 연쇄법칙(chain-rule)을 응용

# 확률론

## 확률론, 통계학의 필요성

- 딥러닝은 확률론 기반
- 머신러닝의 손실함수(loss function)은 데이터 공간을 통계적으로 해석하여 유도
- 회귀분석의 손실함수 L2 norm은 예측오차의 분산값을 최소화 하는 방향으로 학습을 유도
- 분류 문제의 cross-entropy는 모델 예측의 불확실성을 최소화 하는 방향으로 학습을 유도
- 분산, 불확실성의 최소화는 측정 방법을 알아야 가능, 이것을 통계학에서 배움

## 확률분포

- P(X = x)는 x의 값을 가질 확률변수
- 이산형, 연속형을 둘 다 포함하는 형태의 확률분포도 있음

## 조건부확률

- P(y| x) = 입력변수 x에 대해 정답이 y일 확률

## 기대값

- 데이터를 대표하는 통계량
- 확률분포에서 다른 통계적 범함수를 계산하는데 사용
- 기대값을 이용해 분산, 첨도, 공분산 등의 여러 통계량을 계산 가능

## 몬테카를로 샘플링

- 기계학습의 대부분은 확률분포가 명시적으로 확인되지 않는다
- 확률분포를 모를때 데이터로 기대값을 계산 = 몬테카를로 샘플링 방법을 사용
- 독립추출만 보장되면 대수의 법칙에 의해 수렴성이 보장됨

# 통계학

## 모수란(parameter)

- 통계적 모델링 = 적절한 가정 위에서 확률분포를 추정(inference) 하는 것을 목표로
- 유한한 데이터만 관측하여 모집단 분포를 정확히 알아내는 것은 불가능 → 근사적으로 확률분포를 추정하는 것이 최선
- 예측모형: 데이터와 추정법의 불확실성을 고려하여 예측의 위험을 최소화하는 것
- 모수적(parametric) 방법론: 데이터가 특정 확률분포를 따른다 라곤 선험적(a priori) 가정하고, 그 분포를 결정하는 모수(parameter)를 추정하는 방법
- 모수의 예: 정규분포에서의 평균과 분산
- 비모수(nonparametric) 방법론: 특정 확률분포의 가정 없이 데이터에 따라 모델 구조, 모수의 갯수를 유연히 바꾸는 방법
- 머신러닝의 많은 방법론이 비모수 방법론
- 비모수 ≠ 무모수

## 데이터로 모수 추정하기

- 표집분포 ≠ 표본분포

## 최대가능도 추정법

- (WIP)

# 베이즈 통계학 맛보기

## 베이즈 통계학이란

- 데이터가 새로 추가 될 때마다 업데이트 할때 필요한 개념

## 조건부 확률

(WIP)

# CNN 첫걸음

## CNN이란

- fully connected의 가중치 행렬과 달리 작은 규모의 고정된 커널을 입력벡터 상에서 움직여가며 계산하는 구조.
- 선형모델+합성함수 구조는 그대로
- 패러미터 사이즈를 굉장히 많이 줄일 수 있는 방식
- 커널은 f, 신호는 g, 신호가 커널을 거쳐 증폭, 감소
- 사실 convolution(-)이 아닌 cross-correlation(+)이 맞지만, 관행상 convolution으로 부른다
- 커널은 여러가지 → 종류에 따라 영상, 이미지의 처리 결과물이 달라짐
- convolution은 다차원 연산이 가능
- 커널의 데이터 성격에 따라 결과가 달라짐, 커널값은 연산 중에 변하지 않는다

## 2차원 convolution

- 굉장히 많이 사용하는 연산
- 커널은 행렬 형식을 취함
- x,y 좌표 한 칸씩 옮겨가며 순차적으로 계산
- elemental-wise 연산을 한 뒤, 그 값을 모두 더한 것을 결과물 행렬에 넣는다
- 커널값은 역시 연산중에 변함이 없다
- 실전에선 일반적인 컬러 이미지의 rgb 속성 때문에 2D라도 3겹으로 데이터가 형성 → 텐서
- 인풋이 3채널이므로, 커널도 3채널, 3겹의 텐서 구성이 된다
- 커널 연산은 모든 값의 총합, 따라서 3겹의 텐서 연산이라도 결과물은 2차원 행렬이 된다
- 물론 커널을 여러개 사용하면 결과물도 행렬 여러겹의 텐서가 된다
- 역전파 또한 convolution 연산을 적용한다
- 역전파 연산은 원래 계산했던 가중치 순서를 지킨다 → 결과적으로 convolution연산으로 커널값이 업데이트 되는 것

# RNN 첫걸음

## RNN이란

- 시퀀스 데이터 다루기
- 모델 설계 난이도 낮음 → 무엇을 왜 해야 하는지 아는 게 더 중요
- 시퀀스 데이터는 순서가 중요, 순서가 달라지면 그것의 확률분포도 달라짐
    - 개가 사람을 물었다
    - 사람이 개를 물었다
- 앞뒤 맥락없이 미래예측 하기 어려움
- 순차적 정보를 어떻게 다룰지 고민해야 함
- 미래 확률분포 → 베이즈 법칙 활용
- 시퀀스에서 모든 과거 정보는 일반적으로 불필요
- 길이가 가변적 ← 이걸 다룰 모델이 필요
- 고정된 길이만큼만 쓸 경우: 자기 회귀 모델
- 이 길이를 정하는 것도 사전 문제
- 이땐 과거 영향도 있으므로 잘 고려
- 직전 변수 제외한 과거를 잠재변수로 인코딩 → RNN(순환신경망)
- 기본적 RNN 모델은 MLP와 유사
- MLP와 달리 과거정보를 현재변수와 같이 입력
- RNN의 역전파는 시계열 반대순으로 진행 → BPTT

## 고전적 RNN의 문제점

- 최종적으로 전체곱(파이) 연산을 써서 시퀀스가 길 수록 미분값이 극대, 극소화
- 이는 기울기 소실 문제로 직결됨
- 과거로 갈 수록 기울기 소실이 심해짐 → 과거정보를 토대로 미래예측하기 어려워짐
- 때문에 중간에 BPTT를 끊어주는게 중요 → truncated BPTT
- 이런 문제들로 인해 고전적 RNN은 안 씀 → 해결법: LSTM, GRU

---

# 💻과제 해결과정

# 기본과제

- 기본 1
    - Python 내부 math 관련 함수들의 기능을 사용하면 손쉽게 클리어. 특히 numpy를 쓰면 median도 한번에 구할 수 있다.
- 기본 2
    - normalize:
        1. `str.lower()` 함수로 스트링 내 모든 알파벳을 소문자로 전환 
        2. `str.split()`으로 공백 순서와 갯수에 상관없이 공백기준 모든 알파벳 분절을 분리화여 리스트화
        3. `“ ”.join(str)` 으로 공백 1 간격으로 재통합.
    - no_vowels:
        1. `vowels = "aeiouAEIOU"` 같이 대소문자 모음만 모은 스트링 선언
        2. `str.replace(vowels[i], "")` 함수를 활용해 스트링 내 vowels와 같은 원소를 빼버린다.
- 기본 3
    - digits_to_words:
        1. `if` `elif` 문을 활용해 `for` 루프 내의 인풋 스트링의 캐릭터를 하나하나 본다
        2. 만일 캐릭터가 `str.isdigit()` 의 참이면 10가지 숫자 중 대응되는 것의 영문 스트링 + 공백 하나를 `+=` 를 활용하여 concatenate 시킨다
        3. 결과물 스트링의 맨 뒤 공백을 지우기 위해 `str.strip()` 으로 양끝단 공백을 없애고 리턴.
    - to_camel_case:
        1. `str.count('_')` 로 언더스코어의 갯수를 샌 뒤, 하나도 없으면 원본 그대로 리턴, 갯수가 `len(str)` 과 같으면 `""` 스트링을 리턴한다.
        2. `str.lower()` 로 일단 스트링 내 모든 알파벳을 소문자화
        3. `str.split('_')` 로 언더스코어 기준 단어를 나눠 리스트화
        4. `" ".join(str_list)` 로 공백을 넣어 재조립
        5. `str.title()` 로 띄어쓴 뒤 등장하는 맨 첫 알파벳을 대문자화
        6. `str.replace(" ", "")` 으로 스트링 내 공백을 모두 제거
        7. `str[0] = str[0].lower()` 으로 맨 첫 알파벳만 소문자화

# 심화과제

- 심화과제 1: Gradient Descent
    1. Gradient Descent (1)
        1. Function call 순서를 보면 `gradient_descent` → `func_gradient` → `func` 순으로 진행 해야 하는 것을 알 수 있다.
        2. `func` 함수의 `sym.poly.subs` 는 주어진 다항식의 특정 심볼에 지정한 값을 넣어 계산하는 substitute 함수다. 여기서는 x에 `val`을 대입하는 것으로 이용됐다.
        3. `func_gradient`는 최종적으로 기존 함수의 differentiated equation 과 `val`을 대입했을때의 결과값을 리턴해야 한다. 즉, `sym.diff` 를 이용하여 `func`에서 지정된 원본 equation의 미분식을 얻어야 한다.
        4. `gradient_descent`는 `init_point` 부터 시작하여 주어진 함수의 미분값을 학습률에 따라 수정해 나가는 while loop를 구성한다.
    2. Gradient Descent (2)
        1. 위의 코드와 동일한 구조를 짜되, sympy의 `sym.diff` 기능을 이용하지 않고 스스로 구현해보는 문제
        2. 위의 `func_gradient` 함수를 대체하는 `gradient_descent` 함수를 코딩하자
        3. 미분의 기본식인 `(f(x+h) - f(x)) / h`를 그대로 대입하여 함수를 코딩한다
    3. Linear Regression
        1. 3-1 Basic function:
            1.  랜덤한 수열 x와 `func` 에 x 값들을 대입해 얻은 y값으로 w와 b의 gradient를 계산하여 원래 값을 추정해 나가는 작업이다
            2. 예측값 _y는 추정식인 y = w*x + b를 코드로 풀어 쓰면 된다
            3. 각 gradient는  기존에 배웠던 대로 짜면 된다
            4. 그렇게 계산한 gradient들을 각 w와 b에 학습률을 적용하여 `-=` 를 이용해 업데이트한다
            5. 이후 error를 L2 norm으로 계산 해 준다
        2. 3-2 More complicated function:
            1. 더 복잡한 선형식의 regression을 구하는 문제
            2. error를 먼저 구한다
            3. 예측값을 `expand_x`와 `beta_gd` 의 행렬곱으로 구한다 오퍼레이터는 `@`
            4. gradient는 `expand_x` 와 `error` 의 행렬곱으로 구한다. 이 때, expand_x를 transpose 하여 모양을 맞춰야 한다
            5. `beta_gd`를 그렇게 새로 구한 gradient로 업데이트 한다. 단, learning rate를 임의로 정한다. (예: 0.01)
    4. Stochastic Gradient Descent
        1. 3-1과 문제의 기본 골자는 비슷하지만 이번에는 mini batch를 활용한다
        2. `np.random.choice(1000, 10, replace = False)` 를 이용하여 0부터 999 중 10개의 수를 픽업하여 이를 인덱스로 x와 y 데이터셋에 먹여 미니배치를 생성한다
        3. 기존에 풀 배치로 하던 연산을 미니배치로 하도록 코드를 조정하면 된다.
- 심화과제 2: Backpropagation
    1. 학습 가능 파라미터에 대한 loss 함수의 gradient 유도
        
        $$
        \frac{\partial \xi}{\partial W_x} = \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial W_x} + \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial S_{n-1}}\frac{\partial S_{n-1}}{\partial W_x} \cdots = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_k} \frac{\partial S_k}{\partial W_x} = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial S_k} X_k
        $$
        
        $$
        \frac{\partial \xi}{\partial W_{rec}} = \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial W_{rec}} + \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_n} \frac{\partial S_n}{\partial S_{n-1}}\frac{\partial S_{n-1}}{\partial W_{rec}} \cdots = \sum\limits_{k=0}^n \frac{\partial \xi}{\partial y} \frac{\partial y}{\partial S_k} \frac{\partial S_k}{\partial W_{rec}} = \sum\limits_{k=1}^n \frac{\partial \xi}{\partial S_k} S_{k-1}
        $$
        
    2. 시계열상 가장 나중 인덱스부터 0까지 순차적으로 접근하는 For loop를 설계하고, 위의 수식을 코드화 하여 적용한다.
- 심화과제 3: Maximum Likelihood Estimation
    - 미분, log, sum(시그마), product(파이)의 계산법칙을 이해하고 있다는 전제하에, 최대가능도 추정법을 유도해보는 과제다.

---

# 📄피어세션 정리

- 펼쳐보기
    - 나는 모더레이터 역할을 맡아 이번 주 동안 피어세션을 진행했다
    - 아무래도 1주차 세션이라 상당한 시간을 팀 내부 규칙 확립에 투자하였다
    - 이외에도 팀원들과 소소하게 알아가는 시간을 가졌다
    - 학습 진척도가 다들 비슷하여 궁금한 점을 함께 찾아보는 시간도 가졌다

---

# 🤔학습 회고

- 펼쳐보기
    - 새삼 내가 얼마나 안일했는지 깨달았다. 앞으로의 5개월 일정은 절대 널널하지 않다. 오히려 따라가기만도 벅찰 정도로 밀도가 깊다.
    - 선형대수, 확률론, 통계학의 기반지식이 부족하다. 복습을 위해 부스트코스에서 추천한 외부강의들을 주말동안 들을 예정이다.