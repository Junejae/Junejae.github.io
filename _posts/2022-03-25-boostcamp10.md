---
layout: post
title: "[부스트캠프 AI Tech 3기] 10주차 정리 & 회고"
categories:
  - BoostCamp
tags:
  - 부스트캠프
  - AI
  - Deep Learning
  - BERT
  - GPT
  - NLP
  - KLUE
  - Transformer
---
![Untitled](/assets/img/AITech로고.png)

# ✏️학습정리

## 인공지능과 자연어 처리

- AI 탄생과 NLP
    - NLP 소개
        - 과거: Rule-based 기반의 ELIZA 등으로 AI 황금기
        - 현재: 다양한 NLP 응용 분야가 존재, 다른 도메인과 병행하여 서비스 하는 것이 보편적
    - NLP 응용
        - 인간의 NLP: 사전 지식 기반으로 인코딩, 디코딩
        - 컴퓨터 NLP: 자연어를 벡터로 인코딩, 벡터를 기반지식에 따라 자연어로 디코딩
    - 자연어 단어 임베딩
        - 인코딩을 이쁘게 잘 하면 디코딩도 잘 된다 → 좌표 평면 위에 이쁘게 그려지면 분류가 쉬워진다
        - Feature Extraction과 Classification을 스스로 학습하게 하는 것이 기계학습의 원리
        - Word2Vec: 가장 쉬운 건 one-hot 인코딩으로 단어를 벡터로 표현 → sparse representation 문제가 존재 → Word2Vec은 한 단어의 주변 단어를 통해 단어간 관계를 학습
        - FastText: Word2Vec의 단점을 개선, subword information에 집중, n-gram을 이용하여 단어를 분절해, 모든 n-gram 벡터를 합산한 후 평균을 통해 단어 벡터를 획득, 학습, 오탈자, OOV, 마이너 단어 학습에 강세를 보임
        - 둘 다 동형어, 다의어 등의 임베딩 성능이 딸린다, 문맥 고려할 수도 없다.
- DL 기반 NLP와 Language Model
    - Language Model
        - 자연어의 법칙을 컴퓨터로 모사한 모델
        - 주어진 단어로 다음에 등장할 모델을 예측하는 방식으로 학습
        - 고전적인 것은 마코프 체인 모델 → 다음 단어나 문장이 나올 확률을 통계와 단어의 n-gram을 기반으로 계산
        - RNN 기반 모델 → 이전 state 정보를 기반으로 다음 state 예측에 특화
    - Seq2Seq
        - encoder layer와 decoder layer 구조, RNN 베이스
        - 문맥을 고려하여 인코딩과 디코딩을 수행
    - Attention
        - RNN은 구조적으로 Sequence 길이가 길어질 수록 앞자리의 정보가 희석되는 문제가 존재
        - 모든 token이 영향을 미쳐서, 덜 중요한 token도 고려하게 됨
        - Attention: 중간의 히든 스테이트값을 고려하여 동적으로 단어간 중요성을 추산
        - RNN 구조상 시계열 데이터의 입력 순서에 때라 순차적 트레이닝을 하는 것이 비효율적
    - Self-attention
        - 트랜스포머
        - 인코더-디코더가 한 데 묶인 네트워크, RNN의 히든 스테이트 구조가 아닌 병렬 네트워크 채용

## 자연어의 전처리

- 자연어 처리의 단계
    - raw data를 기계학습에 걸맞게 변형하는 것 → 자연어 전처리
    - Task의 성능을 확실하게 올릴 수 있는 방법
    - 저작권에 주의
    - Task 설계 → 필요 데이터 수집 → 통계학적 분석(Token 개수, 아웃라이어 제거, 빈도 확인 사전 정의) → 전처리 → Tagging → Tokenizing(어절, 형태소, WordPiece) → 모델 설계 → 모델 구현 → 성능 평가 → 완료
- Python String 관련 함수
    - 전처리를 위해선 string 함수에 빠삭해야 한다.
    - 
- 한국어 토큰화
    - 한국어는 영어와 달리 토큰화의 난이도가 높다
    - 사용하게 될 pre-trained 모델이 학습에 사용했던 tokenizer를 따라서 사용하는 편 → 다만,

## BERT 언어모델

- Transformer 기반 → masking을 통해 원본 복원을 힘들어지게 만들어 학습능률을 향상
- Input: 2개의 sentence를 [SEP] 토큰을 중간에 넣어 concat
- WordPiece tokenize를 한 데이터 사용해서 학습
- 단일 문장 분류 , 두 문장 관계 분류, 문장 토큰 분류, 기계 독해 정답 분류
- 응용
    - 단일 문장 분류: 감정 분석, 관계 추출
    - 두 문장 관계 분류: 의미 비교(데이터 설계가 잘못됐다는 썰)
    - 문장 토큰 분류: 개체명 분석
    - 기계 독해 정답 분류: 기계 독해
- KoBERT: 형태소 분석 → 워드피스 이중 토크나이징으로 성능 업
- BERT에 Entity 명시 토큰으로 추가 전처리를 해줌으로써 성능 업

## 한국어 BERT 언어모델 학습

- BERT 학습 단계
    - 토크나이저 만들기
    - 데이터셋 확보
    - NSP(Next Sentence Prediction)
    - Masking
- 도메인 특화 task는 그 도메인에 특화된 학습 데이터만 학습에 사용하는 것이 성능이 더 좋다.

## BERT 기반 단일 문장 분류 모델 학습

- KLUE
    - 한국어 자연어 이해 벤치마크
    - 거의 모든 자연어 task 유형을 테스팅 가능
    - 한국어 같이 어순과 생략이 자유로운 언어에서 연구되는 의존 구문 분석도 포함 → 복잡한 자연어 형태를 그래프 구조화로 표현, 각 대상에 대한 정보를 유연히 추출 가능
- 단일 문장 분류 task
    - 주어진 문장이 어느 범주에 속하는지를 구분
    - 감정분석, 주제 라벨링, 언어감지, 의도 분류
- 단일 문장의 맨 앞의 cls 토큰을 분류에 활용하는 방향으로 학습

## BERT 기반 두 문장 관계 분류 모델 학습

- 주어진 두 문장을 concat한 인풋 앞에 분은 cls 토큰으로 관계를 분류하는task
- NLI, semantic text pair, IRQA 등의 task 존재

## BERT 언어모델 기반의 문장 토큰 분류

- 주어진 문장의 각 토큰이 어떤 범주에 속하는지 분류하는 task
- NER(개체명 인식), POS tagging 등이 있다
- pororo - 카카오의 라이브러리, 유용함
- 개체명 인식에선 음절단위 토크나이저를 쓰는 것이 에러가 안 나는 데 도움된다.


# 📄피어세션 정리

- 월: 대회시작, 베이스라인 코드 돌려보기
- 화: 강의 열심히 듣기
- 수:  강의 열심히 듣고 대회 열심히 참가하기
- 목: 오피스 아워, 멘토링
- 금: 논문스터디

# 🤔학습 회고

- 잘했던것, 좋았던것, 계속할것
    - 남의 블로그 글 하나도 안 보고 논문 리뷰 글 써 본 것, 최대한 실험과 논문 근거로 의견제시하려 노력한 것
- 잘못했던것, 아쉬운것, 부족한것 -> 개선방향
    - 대회 실제적 참여가 저조했던 것 → 주말부터라도 열심히 실험 하자.
- 도전할 것, 시도할 것
    - 팀 베이스라인 코드 모듈화, 실험편의성 증대
- 키워드(공부한 것,알게된 것, 느낀 점)
    - 트랜스포머는 혁신적인 아이디어처럼 보여도, 이론적 근간의 확립은 꾸준한 최신 이론 학습이 있었기에 가능했다.